# Pytorch+DLå­¦ä¹ ç¬”è®°

> å°¤å°¼å…‹æ© UNIkeEN
>
> æœ¬ç¬”è®°åŸºäºå¼€æºæ•™å­¦é¡¹ç›®ã€ŠDive-into-DL-PyTorchã€‹ã€SJTU-AI001è¯¾ç¨‹åŠè‡ªå­¦ä¸­çš„æ‰©å±•å†…å®¹

## åŸºæœ¬çŸ¥è¯†ä¸ç®€å•DNNç½‘ç»œ

### çº¿æ€§å›å½’æ¨¡å‹

å³çº¿æ€§æ‹Ÿåˆï¼Œå‡è®¾çº¿æ€§å›å½’å‡è®¾è¾“å‡ºä¸å„ä¸ªè¾“å…¥ä¹‹é—´æ˜¯çº¿æ€§å…³ç³»ï¼š
$$
\hat{y} = x_1 w_1 + x_2 w_2 + b
$$

#### å¹³æ–¹losså‡½æ•°

åœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œæˆ‘ä»¬éœ€è¦è¡¡é‡ä»·æ ¼é¢„æµ‹å€¼ä¸çœŸå®å€¼ä¹‹é—´çš„è¯¯å·®ã€‚é€šå¸¸æˆ‘ä»¬ä¼šé€‰å–ä¸€ä¸ªéè´Ÿæ•°ä½œä¸ºè¯¯å·®ï¼Œä¸”æ•°å€¼è¶Šå°è¡¨ç¤ºè¯¯å·®è¶Šå°ã€‚ä¸€ä¸ªå¸¸ç”¨çš„é€‰æ‹©æ˜¯å¹³æ–¹å‡½æ•°ã€‚å®ƒåœ¨è¯„ä¼°ç´¢å¼•ä¸º $i$ çš„æ ·æœ¬è¯¯å·®çš„è¡¨è¾¾å¼ä¸º

$$\ell^{(i)}(w_1, w_2, b) = \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2$$

å…¶ä¸­å¸¸æ•° $\frac 1 2$ ä½¿å¯¹å¹³æ–¹é¡¹æ±‚å¯¼åçš„å¸¸æ•°ç³»æ•°ä¸º1ï¼Œè¿™æ ·åœ¨å½¢å¼ä¸Šç¨å¾®ç®€å•ä¸€äº›ã€‚è¿™é‡Œä½¿ç”¨çš„å¹³æ–¹è¯¯å·®å‡½æ•°ä¹Ÿç§°ä¸ºå¹³æ–¹æŸå¤±ï¼ˆsquare lossï¼‰ã€‚

é€šå¸¸ï¼Œæˆ‘ä»¬ç”¨è®­ç»ƒæ•°æ®é›†ä¸­æ‰€æœ‰æ ·æœ¬è¯¯å·®çš„å¹³å‡æ¥è¡¡é‡æ¨¡å‹é¢„æµ‹çš„è´¨é‡ï¼Œå³

$$
\ell(w_1, w_2, b) =\frac{1}{n} \sum_{i=1}^n \ell^{(i)}(w_1, w_2, b) =\frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right)^2
$$

åœ¨æ¨¡å‹è®­ç»ƒä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›æ‰¾å‡ºä¸€ç»„æ¨¡å‹å‚æ•°ï¼Œè®°ä¸º $w_1^*, w_2^*, b^*$ï¼Œæ¥ä½¿è®­ç»ƒæ ·æœ¬å¹³å‡æŸå¤±æœ€å°ï¼š

$$
w_1^*, w_2^*, b^* = \underset{w_1, w_2, b}{\arg\min} \ell(w_1, w_2, b)
$$

```python
def squared_loss(y_hat, y): 
    return (y_hat - y.view(y_hat.size())) ** 2 / 2
```

#### losså‡½æ•°åå‘ä¼ æ’­ ä¸ å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™ä¼˜åŒ–ï¼ˆSGD)

å½“æ¨¡å‹å’ŒæŸå¤±å‡½æ•°å½¢å¼è¾ƒä¸ºç®€å•æ—¶ï¼Œä¸Šé¢çš„è¯¯å·®æœ€å°åŒ–é—®é¢˜çš„è§£å¯ä»¥ç›´æ¥ç”¨å…¬å¼è¡¨è¾¾å‡ºæ¥ã€‚è¿™ç±»è§£å«ä½œè§£æè§£ã€‚æœ¬èŠ‚ä½¿ç”¨çš„çº¿æ€§å›å½’å’Œå¹³æ–¹è¯¯å·®åˆšå¥½å±äºè¿™ä¸ªèŒƒç•´ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æ·±åº¦å­¦ä¹ æ¨¡å‹å¹¶æ²¡æœ‰è§£æè§£ï¼Œåªèƒ½é€šè¿‡ä¼˜åŒ–ç®—æ³•æœ‰é™æ¬¡è¿­ä»£æ¨¡å‹å‚æ•°æ¥å°½å¯èƒ½é™ä½æŸå¤±å‡½æ•°çš„å€¼ã€‚è¿™ç±»è§£å«ä½œæ•°å€¼è§£ã€‚

åœ¨æ±‚æ•°å€¼è§£çš„ä¼˜åŒ–ç®—æ³•ä¸­ï¼Œå°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆmini-batch stochastic gradient descentï¼‰åœ¨æ·±åº¦å­¦ä¹ ä¸­è¢«å¹¿æ³›ä½¿ç”¨ã€‚å®ƒçš„ç®—æ³•å¾ˆç®€å•ï¼šå…ˆé€‰å–ä¸€ç»„æ¨¡å‹å‚æ•°çš„åˆå§‹å€¼ï¼Œå¦‚éšæœºé€‰å–ï¼›æ¥ä¸‹æ¥å¯¹å‚æ•°è¿›è¡Œå¤šæ¬¡è¿­ä»£ï¼Œä½¿æ¯æ¬¡è¿­ä»£éƒ½å¯èƒ½é™ä½æŸå¤±å‡½æ•°çš„å€¼ã€‚åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œå…ˆéšæœºå‡åŒ€é‡‡æ ·ä¸€ä¸ªç”±å›ºå®šæ•°ç›®è®­ç»ƒæ•°æ®æ ·æœ¬æ‰€ç»„æˆçš„å°æ‰¹é‡ï¼ˆmini-batchï¼‰$\mathcal{B}$ï¼Œç„¶åæ±‚å°æ‰¹é‡ä¸­æ•°æ®æ ·æœ¬çš„å¹³å‡æŸå¤±æœ‰å…³æ¨¡å‹å‚æ•°çš„å¯¼æ•°ï¼ˆæ¢¯åº¦ï¼‰ï¼Œæœ€åç”¨æ­¤ç»“æœä¸é¢„å…ˆè®¾å®šçš„ä¸€ä¸ªæ­£æ•°çš„ä¹˜ç§¯ä½œä¸ºæ¨¡å‹å‚æ•°åœ¨æœ¬æ¬¡è¿­ä»£çš„å‡å°é‡ã€‚==ï¼ˆå¯¹**loss**å‡½æ•°æ±‚æ¢¯åº¦ï¼Œlossåå‘ä¼ æ’­çš„æ—¶å€™ï¼Œæ¯ä¸€ä¸ªéœ€è¦æ›´æ–°çš„å‚æ•°éƒ½ä¼šæ±‚å‡ºæ¥ä¸€ä¸ªå¯¹åº”çš„æ¢¯åº¦ï¼‰==

> åå‘ä¼ æ’­æŒ‡çš„æ˜¯è®¡ç®—ç¥ç»ç½‘ç»œå‚æ•°æ¢¯åº¦çš„æ–¹æ³•ã€‚æ€»çš„æ¥è¯´ï¼Œåå‘ä¼ æ’­ä¾æ®å¾®ç§¯åˆ†ä¸­çš„é“¾å¼æ³•åˆ™ï¼Œæ²¿ç€ä»è¾“å‡ºå±‚åˆ°è¾“å…¥å±‚çš„é¡ºåºï¼Œä¾æ¬¡è®¡ç®—å¹¶å­˜å‚¨ç›®æ ‡å‡½æ•°æœ‰å…³ç¥ç»ç½‘ç»œå„å±‚çš„ä¸­é—´å˜é‡ä»¥åŠå‚æ•°çš„æ¢¯åº¦ã€‚
>
> **åœ¨è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹æ—¶ï¼Œæ­£å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ä¹‹é—´ç›¸äº’ä¾èµ–ã€‚**
>
> æ¨¡å‹å‚æ•°åˆå§‹åŒ–åï¼Œäº¤æ›¿è¿›è¡Œæ­£å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ï¼Œå¹¶æ ¹æ®åå‘ä¼ æ’­è®¡ç®—çš„æ¢¯åº¦è¿­ä»£æ¨¡å‹å‚æ•°ã€‚åå‘ä¼ æ’­ä¸­ä½¿ç”¨äº†æ­£å‘ä¼ æ’­ä¸­è®¡ç®—å¾—åˆ°çš„ä¸­é—´å˜é‡æ¥é¿å…é‡å¤è®¡ç®—ï¼Œé‚£ä¹ˆè¿™ä¸ªå¤ç”¨ä¹Ÿå¯¼è‡´æ­£å‘ä¼ æ’­ç»“æŸåä¸èƒ½ç«‹å³é‡Šæ”¾ä¸­é—´å˜é‡å†…å­˜ã€‚è¿™ä¹Ÿæ˜¯è®­ç»ƒè¦æ¯”é¢„æµ‹å ç”¨æ›´å¤šå†…å­˜çš„ä¸€ä¸ªé‡è¦åŸå› ã€‚
>
> ä¸­é—´å˜é‡ä¸ªæ•°ä¸ç½‘ç»œå±‚æ•°çº¿æ€§ç›¸å…³ï¼Œå˜é‡å¤§å°ã€æ‰¹é‡å¤§å°ä¸è¾“å…¥ä¸ªæ•°ä¹Ÿæ˜¯çº¿æ€§ç›¸å…³çš„ï¼Œå®ƒä»¬æ˜¯å¯¼è‡´è¾ƒæ·±çš„ç¥ç»ç½‘ç»œä½¿ç”¨è¾ƒå¤§æ‰¹é‡è®­ç»ƒæ—¶æ›´å®¹æ˜“è¶…å†…å­˜çš„ä¸»è¦åŸå› ã€‚

åœ¨è®­ç»ƒæœ¬èŠ‚è®¨è®ºçš„çº¿æ€§å›å½’æ¨¡å‹çš„è¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹çš„æ¯ä¸ªå‚æ•°å°†ä½œå¦‚ä¸‹è¿­ä»£ï¼š

$$
\begin{aligned}
w_1 &\leftarrow w_1 -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \frac{ \partial \ell^{(i)}(w_1, w_2, b)  }{\partial w_1} = w_1 -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_1^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right),\\
w_2 &\leftarrow w_2 -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \frac{ \partial \ell^{(i)}(w_1, w_2, b)  }{\partial w_2} = w_2 -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_2^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right),\\
b &\leftarrow b -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \frac{ \partial \ell^{(i)}(w_1, w_2, b)  }{\partial b} = b -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}\left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right).
\end{aligned}
$$

åœ¨ä¸Šå¼ä¸­ï¼Œ$|\mathcal{B}|$ ä»£è¡¨æ¯ä¸ªå°æ‰¹é‡ä¸­çš„æ ·æœ¬ä¸ªæ•°ï¼ˆæ‰¹é‡å¤§å°ï¼Œbatch sizeï¼‰ï¼Œ$\eta$ ç§°ä½œå­¦ä¹ ç‡å¹¶å–æ­£æ•°ã€‚

è¿™é‡Œçš„æ‰¹é‡å¤§å°å’Œå­¦ä¹ ç‡çš„å€¼æ˜¯äººä¸ºè®¾å®šçš„ï¼Œå¹¶ä¸æ˜¯é€šè¿‡æ¨¡å‹è®­ç»ƒå­¦å‡ºçš„ï¼Œå› æ­¤å«ä½œè¶…å‚æ•°ï¼‰ã€‚æˆ‘ä»¬é€šå¸¸æ‰€è¯´çš„â€œè°ƒå‚â€æŒ‡çš„æ­£æ˜¯è°ƒèŠ‚è¶…å‚æ•°ï¼Œä¾‹å¦‚é€šè¿‡åå¤è¯•é”™æ¥æ‰¾åˆ°è¶…å‚æ•°åˆé€‚çš„å€¼ã€‚åœ¨å°‘æ•°æƒ…å†µä¸‹ï¼Œè¶…å‚æ•°ä¹Ÿå¯ä»¥é€šè¿‡æ¨¡å‹è®­ç»ƒå­¦å‡ºã€‚ 

`torch.optim`æ¨¡å—æä¾›å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰

```python
def sgd(params, lr, batch_size): 
    for param in params:
        param.data -= lr * param.grad / batch_size # æ³¨æ„è¿™é‡Œæ›´æ”¹paramæ—¶ç”¨çš„param.data
```

SGDä¼˜ç‚¹ï¼š

* ä¸æ˜¯å¯¹å…¨éƒ¨æ•°æ®è¿›è¡Œè¿ç®—ï¼Œè¿­ä»£é€Ÿåº¦å¤§å¤§åŠ å¿«

SGDç¼ºç‚¹ï¼š

* å•ä¸ªæ ·æœ¬çš„æ¢¯åº¦å¹¶éå…¨å±€æœ€ä¼˜ï¼Œä¼˜åŒ–æ–¹å‘å‡†ç¡®åº¦ä¸å¤Ÿ
* ä¸æ˜“äºå¹¶è¡Œå®ç°

è€ŒåŸå§‹æ¢¯åº¦ä¸‹é™ï¼ˆBDGï¼Œæœ€åŸå§‹çš„å½¢å¼ã€‚æ¯æ¬¡è¿­ä»£æ—¶ä½¿ç”¨æ‰€æœ‰æ ·æœ¬è¿›è¡Œæ¢¯åº¦æ›´æ–°ï¼‰å®ç°å¹¶è¡ŒåŠ é€Ÿã€æ›´æ–°æ–¹å‘å‡†ç¡®ï¼Œä½†åœ¨æ ·æœ¬æ•°ç›®è¿‡å¤§æ—¶è®­ç»ƒå¾ˆæ…¢ï¼Œä¸”å®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜

#### æ‰‹å†™å®ç°

==\1_Linear_Regression.ipynb==

```python
import torch
import numpy as np
import random
from matplotlib import pyplot as plt
```

##### éšæœºç”Ÿæˆæ•°æ®é›†

```python
num_inputs=3
num_examples=500
real_w=[-6, 3.2, 10.5]  # w1,w2,w3çœŸå®å€¼
real_b=1.7  # bçœŸå®å€¼

# ç”Ÿæˆéšæœºè¾“å…¥ï¼Œä¸º10000*3çŸ©é˜µ
features=torch.randn(num_examples,num_inputs,dtype=torch.float32)

# æ ¹æ®çœŸå®å€¼ç”Ÿæˆæ ‡ç­¾
labels = real_w[0] * features[:,0] 
for i in range(1,num_inputs):
    labels += real_w[i]*features[:,i]
labels += real_b

# ç»™æ ‡ç­¾åŠ ä¸Šå™ªå£°
labels+=torch.tensor(np.random.normal(0,0.01,size=labels.size()),dtype=torch.float32)
```

##### è¯»å–æ•°æ®

```python
def data_iter(batch_size,features,labels):
    num_examples=len(features)
    
    # ç”Ÿæˆå¤§å°ä¸ºnum_examplesçš„æ•°å€¼åˆ—è¡¨å¹¶æ‰“ä¹±ï¼Œåšåˆ°é¡ºåºéšæœºè¯»å–æ•°æ®
    index=list(range(num_examples)) 
    random.shuffle(index)
    
    for i in range(0, num_examples, batch_size):
        j=torch.LongTensor(index[i:min(i+batch_size,num_examples)]) # æ­¤å¤„LongTensorç›¸å½“äºç´¢å¼•å¼ é‡
        
        # ä½¿ç”¨index_select(dim,index) åœ¨dimç»´å–åºå·ä¸ºindexçš„æ•°æ®
        # yieldä¸­æ–­ï¼Œä¿è¯æŒ‰éšå³åˆ—è¡¨å…¨éƒ¨å–å®Œ
        yield features.index_select(0,j), labels.index_select(0,j)  
```

##### åˆå§‹åŒ–æ¨¡å‹å‚æ•°

```python
w=torch.tensor(np.random.normal(0,0.01,(num_inputs,1)),dtype=torch.float32) # ç”Ÿæˆ3*1çŸ©é˜µä¾¿äºä¹˜æ³•
b=torch.zeros(1,dtype=torch.float32)

# è¦æ±‚æ¢¯åº¦è¿½è¸ª
w.requires_grad_(requires_grad=True)
b.requires_grad_(requires_grad=True)
```

##### å®šä¹‰æ¨¡å‹

```python
def net(X,W,b):
    return torch.mm(X , W)+ b
```

##### å®šä¹‰losså‡½æ•°

```python
def loss(y,real_y):
    return (y-real_y.view(y.size()))**2/2
```

##### å®šä¹‰sgdå‡½æ•°

```python
def sgd(params, lr, batch_size):
    for param in params:
        param.data -= lr * param.grad / batch_size
```

##### è®­ç»ƒæ¨¡å‹

```python
num_epochs=7
lr=0.03
batch_size=10

for epoch in range(num_epochs):
    for X, Y in data_iter(batch_size, features, labels):
        l=loss(net(X,w,b),Y).sum() # Yæ˜¯æ•°æ®é›†ä¸­çš„è¾“å‡ºï¼Œä¸netç»“æœç›¸æ¯”è¾ƒã€‚ä½¿ç”¨sumå‡½æ•°æ±‚å’Œè½¬ä¸ºæ ‡é‡ï¼Œä¾¿äºbackwardè®¡ç®—
        l.backward() # è®¡ç®—losså‡½æ•°çš„æ¢¯åº¦
        sgd([w,b],lr,batch_size)

        #æ¢¯åº¦æ¸…é›¶ï¼ˆå®é™…ä¸Šåº”è¯¥å…ˆæ¸…æ¢¯åº¦ï¼Ÿä½†åœ¨æ‰‹å†™å®ç°æ—¶è¦æ³¨æ„æ˜¯å¦ä¸ºNoneï¼‰
        w.grad.data.zero_()
        b.grad.data.zero_()
    epoch_l=loss(net(features,w,b),labels)
    print('epoch %d, loss %f'%(epoch+1,epoch_l.mean().item())) #meanæ±‚å‡å€¼ï¼Œitemå°†tensorè½¬ä¸ºæ•°

# è¾“å‡ºç»“æœ
print('\n', real_w,'\n',w)
print(real_b,'\n',b)
```



#### Pytorchç®€æ´å®ç°

==\2_Linear_Regression_Pytorch.ipynb==

```python
import torch
from torch import nn
import numpy as np
torch.manual_seed(1) #ç”Ÿæˆéšæœºæ•°ç§å­

torch.set_default_tensor_type('torch.FloatTensor')
```

##### éšæœºç”Ÿæˆæ•°æ®é›†ï¼ˆåŒä¸ŠèŠ‚ï¼‰

```python
num_inputs=3
num_examples=1000
real_w=[-6, 3.2, 10.5]  # w1,w2,w3çœŸå®å€¼
real_b=1.7  # bçœŸå®å€¼

# ç”Ÿæˆéšæœºè¾“å…¥ï¼Œä¸º1000*3çŸ©é˜µ
features=torch.randn(num_examples,num_inputs,dtype=torch.float32)

# æ ¹æ®çœŸå®å€¼ç”Ÿæˆæ ‡ç­¾
labels = real_w[0] * features[:,0] 
for i in range(1,num_inputs):
    labels += real_w[i]*features[:,i]
labels += real_b

# ç»™æ ‡ç­¾åŠ ä¸Šå™ªå£°
labels+=torch.tensor(np.random.normal(0,0.01,size=labels.size()),dtype=torch.float32)
```

##### è¯»å–æ•°æ®

`torch.utils.data.TensorDataset ` å¯ä»¥å°†è®­ç»ƒé›†çš„ç‰¹å¾å’Œæ ‡ç­¾é…å¯¹æ‰“åŒ…

`torch.utils.data.DataLoader ` å¯ä»¥æä¾›ä»¥Batchä¸ºå•ä½çš„æ•°æ®è¯»å–å‡½æ•°ï¼Œshuffle=Trueè¡¨ç¤ºéšå³é¡ºåº

```python
import torch.utils.data as Data

batch_size=10

# å°†è®­ç»ƒæ•°æ®çš„ç‰¹å¾å’Œæ ‡ç­¾æ‰“åŒ…æˆç»„åˆ
dataset=Data.TensorDataset(features,labels)

# éšæœºæŒ‰Batchè¯»å–ï¼ˆä»£æ›¿æ‰‹å†™iterï¼‰shuffle=Trueè¡¨ç¤ºéšæœºé¡ºåºè¯»å–
data_iter=Data.DataLoader(dataset,batch_size,shuffle=True)
```

##### å®šä¹‰æ¨¡å‹ç½‘ç»œ

pytorché€šè¿‡`torch.nn`æ¨¡å—æä¾›äº†å¤§é‡é¢„å®šä¹‰å±‚ï¼Œ`nn`çš„æ ¸å¿ƒæ•°æ®ç»“æ„æ˜¯`Module`

å®é™…ä½¿ç”¨æ—¶å¯ä»¥ç»§æ‰¿`nn.Moudle`æ„å»ºè‡ªå·±çš„ç½‘ç»œ/å±‚ï¼Œä¹Ÿå¯ä»¥é€šè¿‡pytorchæä¾›çš„æœ‰åºå®¹å™¨`nn.Sequential`æ­å»ºç½‘ç»œï¼Œç½‘ç»œå±‚æŒ‰ç…§ä¼ å…¥`nn.Sequential`çš„é¡ºåºåŠ å…¥è®¡ç®—å›¾ä¸­

```python
# æ–¹æ³•ä¸€ï¼Œç»§æ‰¿nn.Moudleè‡ªå®šä¹‰ç½‘ç»œ
class LinearNet(nn.Module):
    def __init__(self, n_feature):
        super(LinearNet, self).__init__()
        self.linear = nn.Linear(n_feature, 1)
    # forward å®šä¹‰å‰å‘ä¼ æ’­
    def forward(self, x):
        y = self.linear(x)
        return y
    
net = LinearNet(num_inputs)
# print(net) ä½¿ç”¨printå¯ä»¥æ‰“å°å‡ºç½‘ç»œçš„ç»“æ„
```

```python
# æ–¹æ³•2 åˆ©ç”¨nn.Sequentialå®¹å™¨é¡ºåºåŠ å…¥å±‚
net=nn.Sequential(
    nn.Linear(num_inputs,1) # è¾“å…¥num_inputsä¸ªæ•°æ®ï¼Œè¾“å‡º1ä¸ªæ•°æ®ï¼ˆæŒ‡yï¼‰
    # æ³¨æ„ï¼Œè¾“å‡ºçš„ä¸æ˜¯å‚æ•°ï¼Œæ˜¯y=wx+bçš„yï¼æ‰€ä»¥æ˜¯1ä¸ª
)
```

##### åˆå§‹åŒ–æ¨¡å‹å‚æ•°

`nn`ä¸­æä¾›äº†`init`åº“ï¼Œå¯ä»¥ä»¥æ­£æ€ã€éšæœºã€å¸¸é‡ç­‰æ–¹å¼åˆå§‹åŒ–å˜é‡

```python
from torch.nn import init

init.normal_(net[0].weight, mean=0, std=0.01) # wåˆå§‹åŒ–ä¸ºéšæœºé‡‡æ ·å‡å€¼0ï¼Œæ ‡å‡†å·®0.01çš„æ­£æ€åˆ†å¸ƒ
init.constant_(net[0].bias, val=0) # båˆå§‹åŒ–ä¸º0
```

##### å®šä¹‰losså‡½æ•°

```python
loss=nn.MSELoss()
```

##### å®šä¹‰sgdå‡½æ•°ï¼ˆä¼˜åŒ–å™¨ï¼‰

`torch.optim`ä¸­æœ‰å¯é€‰å†…ç½®ä¼˜åŒ–å™¨ï¼Œæ­¤å¤„ä½¿ç”¨SGD

```python
import torch.optim as optim

optimizer=optim.SGD(net[0].parameters(),lr=0.03)
```

##### è®­ç»ƒæ¨¡å‹

==å…ˆå°†ä¼˜åŒ–å™¨æ¸…é›¶ï¼Œå†å°†losså‡½æ•°åå‘ä¼ æ’­ï¼Œå†è¿­ä»£ä¼˜åŒ–å™¨==

> gradåœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­æ˜¯ç´¯åŠ çš„(accumulated)ï¼Œè¿™æ„å‘³ç€æ¯ä¸€æ¬¡è¿è¡Œåå‘ä¼ æ’­ï¼Œæ¢¯åº¦éƒ½ä¼šç´¯åŠ ä¹‹å‰çš„æ¢¯åº¦ï¼Œæ‰€ä»¥ä¸€èˆ¬åœ¨åå‘ä¼ æ’­ä¹‹å‰éœ€æŠŠæ¢¯åº¦æ¸…é›¶ã€‚

```python
num_epochs=3
for epoch in range(1,num_epochs+1):
    for x,y in data_iter:
        output=net(x)
        l=loss(output, y.view(-1,1))  
        	# y.view(-1,1)æ”¹å˜yçš„å½¢çŠ¶ï¼Œ1è¡¨ç¤ºæ”¹ä¸º1åˆ—ï¼Œ-1è¡¨ç¤ºæ ¹æ®å…¶ä»–ç»´åº¦ï¼ˆæ­¤å¤„ä¸ºè¡Œï¼‰æ¨æ–­æœ¬ç»´åº¦ï¼ˆåˆ—æ•°ï¼‰
        optimizer.zero_grad() #æ¢¯åº¦æ¸…é›¶ï¼Œç­‰ä»·äºnet.zero_grad()
        l.backward()
        optimizer.step() #è¿­ä»£æ¨¡å‹å‚æ•°
    print('epoch %d, loss: %f' % (epoch, l.item()))

# è¾“å‡ºç»“æœ
print('\n', real_w,'\n',net[0].weight)
print(real_b,'\n',net[0].bias)
```



***

### Softmaxå›å½’æ¨¡å‹ä¸åˆ†ç±»é—®é¢˜

Softmaxå›å½’ä¸çº¿æ€§å›å½’æ¨¡å‹åŒä¸ºå•å±‚ç¥ç»ç½‘ç»œï¼Œä½†softmaxå›å½’çš„è¾“å‡ºå•å…ƒä»ä¸€ä¸ªå˜æˆäº†å¤šä¸ªï¼Œå¼•å…¥äº†softmaxè¿ç®—ä½¿è¾“å‡ºé€‚åˆç¦»æ•£å€¼çš„é¢„æµ‹å’Œè®­ç»ƒï¼Œä¸»è¦ç”¨äºå¤šåˆ†ç±»æ¨¡å‹çš„è¾“å‡ºã€‚

softmaxå›å½’è·Ÿçº¿æ€§å›å½’ä¸€æ ·å°†è¾“å…¥ç‰¹å¾ä¸æƒé‡åšçº¿æ€§å åŠ ã€‚ä¸çº¿æ€§å›å½’çš„ä¸€ä¸ªä¸»è¦ä¸åŒåœ¨äºï¼Œsoftmaxå›å½’çš„è¾“å‡ºå€¼ä¸ªæ•°ç­‰äºæ ‡ç­¾é‡Œçš„ç±»åˆ«æ•°ã€‚å¦‚æœ‰4ç§ç‰¹å¾å’Œ3ç§è¾“å‡ºç±»åˆ«ï¼Œæ‰€ä»¥æƒé‡åŒ…å«12ä¸ªæ ‡é‡ï¼ˆå¸¦ä¸‹æ ‡çš„$w$ï¼‰ã€åå·®åŒ…å«3ä¸ªæ ‡é‡ï¼ˆå¸¦ä¸‹æ ‡çš„$b$ï¼‰ï¼Œä¸”å¯¹æ¯ä¸ªè¾“å…¥è®¡ç®—$o_1, o_2, o_3$è¿™3ä¸ªè¾“å‡ºï¼š
$$
\begin{aligned}
o_1 &= x_1 w_{11} + x_2 w_{21} + x_3 w_{31} + x_4 w_{41} + b_1,\\
o_2 &= x_1 w_{12} + x_2 w_{22} + x_3 w_{32} + x_4 w_{42} + b_2,\\
o_3 &= x_1 w_{13} + x_2 w_{23} + x_3 w_{33} + x_4 w_{43} + b_3.
\end{aligned}
$$
![3.4_softmaxreg](figs/3.4_softmaxreg.svg)

#### Softmaxè¿ç®—ç¬¦

åˆ†ç±»é—®é¢˜éœ€è¦ç¦»æ•£çš„é¢„æµ‹è¾“å‡ºï¼Œå¯ä»¥å°†è¾“å‡ºå€¼$o_i$ä½œä¸ºç±»åˆ«$i$çš„ç½®ä¿¡åº¦ï¼Œå¹¶å°†å€¼æœ€å¤§çš„è¾“å‡ºä½œä¸ºé¢„æµ‹çš„ç±»åˆ«è¾“å‡ºã€‚ä¸ºäº†**ä¾¿äºå°†è¾“å‡ºç»“æœå’ŒçœŸå®æ ‡ç­¾æ¯”è¾ƒã€å¢åŠ åŒºåˆ†å¯¹æ¯”åº¦ä½¿å­¦ä¹ æ•ˆç‡æ›´é«˜**ï¼Œå¼•å…¥softmaxè¿ç®—ç¬¦
$$
\hat{y}_1, \hat{y}_2, \hat{y}_3 = \text{softmax}(o_1, o_2, o_3)
$$

å…¶ä¸­

$$
\hat{y}_1 = \frac{ \exp(o_1)}{\sum_{i=1}^3 \exp(o_i)},\quad
\hat{y}_2 = \frac{ \exp(o_2)}{\sum_{i=1}^3 \exp(o_i)},\quad
\hat{y}_3 = \frac{ \exp(o_3)}{\sum_{i=1}^3 \exp(o_i)}.
$$

softmaxè¿ç®—ç¬¦ä¼˜ç‚¹å’Œä½œç”¨æ€»ç»“ï¼š

* å°†è¾“å‡ºå€¼å½’ä¸€åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼Œä¾¿äºä¸çœŸå®æ ‡ç­¾æ¯”è¾ƒã€‚
* å¤§çš„æ›´å¤§ï¼Œå°çš„æ›´å°ï¼Œå¢åŠ åŒºåˆ†å¯¹æ¯”åº¦ï¼Œæå‡å­¦ä¹ æ•ˆç‡ã€‚
* softmaxè¿ç»­å¯å¯¼ï¼Œåœ¨æœºå™¨å­¦ä¹ çš„æ¢¯åº¦ä¸‹é™æ³•ä¸­éå¸¸å¿…è¦ã€‚

#### æ ·æœ¬åˆ†ç±»çŸ¢é‡è®¡ç®—

é€šå¸¸å¯¹å°æ‰¹é‡æ•°æ®åšçŸ©é˜µè¿ç®—ã€‚å¹¿ä¹‰ä¸Šè®²ï¼Œç»™å®šä¸€ä¸ªå°æ‰¹é‡æ ·æœ¬ï¼Œå…¶æ‰¹é‡å¤§å°ä¸º$n$ï¼Œè¾“å…¥ä¸ªæ•°ï¼ˆç‰¹å¾æ•°ï¼‰ä¸º$d$ï¼Œè¾“å‡ºä¸ªæ•°ï¼ˆç±»åˆ«æ•°ï¼‰ä¸º$q$ã€‚è®¾æ‰¹é‡ç‰¹å¾ä¸º$\boldsymbol{X} \in \mathbb{R}^{n \times d}$ã€‚å‡è®¾softmaxå›å½’çš„æƒé‡å’Œåå·®å‚æ•°åˆ†åˆ«ä¸º$\boldsymbol{W} \in \mathbb{R}^{d \times q}$å’Œ$\boldsymbol{b} \in \mathbb{R}^{1 \times q}$ã€‚softmaxå›å½’çš„çŸ¢é‡è®¡ç®—è¡¨è¾¾å¼ä¸º

$$
\begin{aligned}
\boldsymbol{O} &= \boldsymbol{X} \boldsymbol{W} + \boldsymbol{b},\\
\boldsymbol{\hat{Y}} &= \text{softmax}(\boldsymbol{O}),
\end{aligned}
$$

#### äº¤å‰ç†µlosså‡½æ•°

åˆ†ç±»é—®é¢˜ä¸­ï¼ŒçœŸå®çš„æ ‡ç­¾ç”¨ç±»åˆ«åˆ†å¸ƒè¡¨ç¤ºä¸ºä¸€ä¸ª1ï¼Œå…¶ä½™å‡ä¸º0ã€‚

æˆ‘ä»¬å¯ä»¥åƒçº¿æ€§å›å½’ä¸€æ ·ä½¿ç”¨å¹³æ–¹æŸå¤±å‡½æ•°ï¼Œä½†æƒ³è¦é¢„æµ‹åˆ†ç±»ç»“æœæ­£ç¡®ï¼Œå¹¶ä¸éœ€è¦é¢„æµ‹æ¦‚ç‡å®Œå…¨æ¥è¿‘æ ‡ç­¾æ¦‚ç‡ã€‚æ¯”å¦‚å›¾åƒåˆ†ç±»æ—¶ï¼Œåªéœ€è¦çœŸå®åˆ†ç±»çš„é¢„æµ‹å€¼æ¯”å…¶ä½™åˆ†ç±»çš„é¢„æµ‹å€¼å¤§å°±è¡Œäº†ã€‚è€Œå¹³æ–¹æŸå¤±è¿‡äºä¸¥æ ¼ï¼Œä¸ºæ­¤å¼•å…¥ä¸€ä¸ªæ›´é€‚åˆè¡¡é‡ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒå·®å¼‚çš„æµ‹é‡å‡½æ•°â€”â€”äº¤å‰ç†µ

$$
H\left(\boldsymbol y^{(i)}, \boldsymbol {\hat y}^{(i)}\right ) = -\sum_{j=1}^q y_j^{(i)} \log \hat y_j^{(i)}
$$

$\hat y$ä¸ºé¢„æµ‹å€¼ï¼Œ$y$ä¸ºçœŸå®å€¼ã€‚

å‡è®¾è®­ç»ƒæ•°æ®é›†çš„æ ·æœ¬æ•°ä¸º$n$ï¼Œäº¤å‰ç†µæŸå¤±å‡½æ•°å®šä¹‰ä¸º
$$
\ell(\boldsymbol{\Theta}) = \frac{1}{n} \sum_{i=1}^n H\left(\boldsymbol y^{(i)}, \boldsymbol {\hat y}^{(i)}\right )
$$

å…¶ä¸­$\boldsymbol{\Theta}$ä»£è¡¨æ¨¡å‹å‚æ•°ã€‚åŒæ ·åœ°ï¼Œå¦‚æœæ¯ä¸ªæ ·æœ¬åªæœ‰ä¸€ä¸ªæ ‡ç­¾ï¼Œé‚£ä¹ˆäº¤å‰ç†µæŸå¤±å¯ä»¥ç®€å†™æˆ$\ell(\boldsymbol{\Theta}) = -\frac{1}{n}  \sum_{i=1}^n \log \hat y_{y^{(i)}}^{(i)}$ã€‚ä»å¦ä¸€ä¸ªè§’åº¦æ¥çœ‹ï¼Œæˆ‘ä»¬çŸ¥é“æœ€å°åŒ–$\ell(\boldsymbol{\Theta})$ç­‰ä»·äºæœ€å¤§åŒ–$\exp(-n\ell(\boldsymbol{\Theta}))=\prod_{i=1}^n \hat y_{y^{(i)}}^{(i)}$ï¼Œå³==æœ€å°åŒ–äº¤å‰ç†µæŸå¤±å‡½æ•°ç­‰ä»·äºæœ€å¤§åŒ–è®­ç»ƒæ•°æ®é›†æ‰€æœ‰æ ‡ç­¾ç±»åˆ«çš„è”åˆé¢„æµ‹æ¦‚ç‡ï¼ˆï¼Ÿï¼‰==ã€‚

#### å‡†ç¡®ç‡

åœ¨è®­ç»ƒå¥½softmaxå›å½’æ¨¡å‹åï¼Œç»™å®šä»»ä¸€æ ·æœ¬ç‰¹å¾ï¼Œå°±å¯ä»¥é¢„æµ‹æ¯ä¸ªè¾“å‡ºç±»åˆ«çš„æ¦‚ç‡ã€‚é€šå¸¸ï¼Œæˆ‘ä»¬æŠŠé¢„æµ‹æ¦‚ç‡æœ€å¤§çš„ç±»åˆ«ä½œä¸ºè¾“å‡ºç±»åˆ«ã€‚å¦‚æœå®ƒä¸çœŸå®ç±»åˆ«ï¼ˆæ ‡ç­¾ï¼‰ä¸€è‡´ï¼Œè¯´æ˜è¿™æ¬¡é¢„æµ‹æ˜¯æ­£ç¡®çš„ã€‚

æˆ‘ä»¬å°†ä½¿ç”¨å‡†ç¡®ç‡ï¼ˆaccuracyï¼‰æ¥è¯„ä»·æ¨¡å‹çš„è¡¨ç°ã€‚å®ƒç­‰äºæ­£ç¡®é¢„æµ‹æ•°é‡ä¸æ€»é¢„æµ‹æ•°é‡ä¹‹æ¯”ã€‚

####  å›¾åƒåˆ†ç±»æ•°æ®é›† Fashion-MNIST

æœ€å¸¸ç”¨ï¼šæ‰‹å†™æ•°å­—è¯†åˆ«æ•°æ®é›†MNIST

å›¾åƒå†…å®¹æ›´å¤æ‚çš„æ•°æ®é›†Fashion-MNIST

` torchvision.transforms.ToTensor()`ä½¿æ‰€æœ‰æ•°æ®è½¬æ¢ä¸º`Tensor`ï¼Œå¦‚æœä¸è¿›è¡Œè½¬æ¢åˆ™è¿”å›çš„æ˜¯PILå›¾ç‰‡ã€‚`transforms.ToTensor()`å°†å°ºå¯¸ä¸º (H x W x C) ä¸”æ•°æ®ä½äº[0, 255]çš„PILå›¾ç‰‡æˆ–è€…æ•°æ®ç±»å‹ä¸º`np.uint8`çš„NumPyæ•°ç»„è½¬æ¢ä¸ºå°ºå¯¸ä¸º(C x H x W)ä¸”æ•°æ®ç±»å‹ä¸º`torch.float32`ä¸”ä½äº[0.0, 1.0]çš„`Tensor`ã€‚

> æ³¨æ„ï¼š ç”±äºåƒç´ å€¼ä¸º0åˆ°255çš„æ•´æ•°ï¼Œæ‰€ä»¥åˆšå¥½æ˜¯uint8æ‰€èƒ½è¡¨ç¤ºçš„èŒƒå›´ï¼ŒåŒ…æ‹¬`transforms.ToTensor()`åœ¨å†…çš„ä¸€äº›å…³äºå›¾ç‰‡çš„å‡½æ•°é»˜è®¤è¾“å…¥çš„æ˜¯uint8å‹ï¼Œè‹¥ä¸æ˜¯ï¼Œå¯èƒ½ä¸ä¼šæŠ¥é”™ä½†å¯èƒ½å¾—ä¸åˆ°æƒ³è¦çš„ç»“æœã€‚æ‰€ä»¥ï¼Œ**å¦‚æœç”¨åƒç´ å€¼(0-255æ•´æ•°)è¡¨ç¤ºå›¾ç‰‡æ•°æ®ï¼Œé‚£ä¹ˆä¸€å¾‹å°†å…¶ç±»å‹è®¾ç½®æˆuint8ï¼Œé¿å…ä¸å¿…è¦çš„bugã€‚** 

é€šè¿‡ä»¥ä¸‹`torchvision`å†…ç½®å‡½æ•°å¯ä»¥è°ƒç”¨è¿™ä¸€æ•°æ®é›†

```python
mnist_train = torchvision.datasets.FashionMNIST(root='~/Datasets/FashionMNIST', train=True, download=True, transform=transforms.ToTensor())
mnist_test = torchvision.datasets.FashionMNIST(root='~/Datasets/FashionMNIST', train=False, download=True, transform=transforms.ToTensor())
```

`train`å‚æ•°æ ‡è®°æ˜¯è®­ç»ƒé›†è¿˜æ˜¯æµ‹è¯•é›†

#### æ‰‹å†™å®ç°

==\3_Softmax_Regression_SLP.ipynb==

```python
import torch
import torchvision
from torch import nn
from torch.nn import init
import numpy as np
import sys
```

##### ä»Fashion-MINSTè¯»å–æ•°æ®

è°ƒç”¨API

```python
def load_data_fashion_mnist(batch_size, resize=None, root='~/Datasets/FashionMNIST'):
    """Download the fashion mnist dataset and then load into memory."""
    trans = []
    if resize:
        trans.append(torchvision.transforms.Resize(size=resize))
    trans.append(torchvision.transforms.ToTensor())
    
    transform = torchvision.transforms.Compose(trans)
    mnist_train = torchvision.datasets.FashionMNIST(root=root, train=True, download=True, transform=transform)
    mnist_test = torchvision.datasets.FashionMNIST(root=root, train=False, download=True, transform=transform)
    if sys.platform.startswith('win'):
        num_workers = 0  # 0è¡¨ç¤ºä¸ç”¨é¢å¤–çš„è¿›ç¨‹æ¥åŠ é€Ÿè¯»å–æ•°æ®
    else:
        num_workers = 4
    train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)
    test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)

    return train_iter, test_iter
```

```python
batch_size = 256
train_iter, test_iter = load_data_fashion_mnist(batch_size)
```

##### åˆå§‹åŒ–æ¨¡å‹å‚æ•°

å·²çŸ¥æ ·æœ¬è¾“å…¥ä¸º28Ã—28åƒç´ ï¼Œå…±10ä¸ªç±»åˆ«ã€‚åˆ™softmaxå›å½’çš„w,båˆ†åˆ«ä¸º784Ã—10å’Œ1Ã—10çŸ©é˜µ

```python
num_inputs=784
num_outputs=10

w=torch.tensor(np.random.normal(0,0.01,(num_inputs,num_outputs)),dtype=torch.float)
b=torch.zeros(num_outputs,dtype=torch.float)

w.requires_grad_(requires_grad=True)
b.requires_grad_(requires_grad=True)
```

##### å®ç°softmaxè¿ç®—

```python
def softmax(x):
    x_exp=x.exp()
    tot=x_exp.sum(dim=1,keepdim=True) #è¡¨ç¤ºå¯¹ç¬¬1ç»´ï¼ˆè¡Œï¼‰æ±‚å’Œä¸”ä¿æŒç»´åº¦
    return x_exp / tot
```

##### å®šä¹‰æ¨¡å‹

```python
def net(x):
    return softmax(torch.mm(x.view(-1,num_inputs),w)+b)

```

æ­¤å¤„ä¼ å…¥(batchsize,1,28,28),è½¬ä¸º(batchsize,784)
å’Œwç›¸ä¹˜å(batchsize,10)+b(1,10)ï¼Œåˆ™å‰è€…æ¯ä¸€è¡Œéƒ½ä¼šåŠ ä¸Šbè¿™ä¸€è¡Œ

##### å®šä¹‰losså‡½æ•°

å•æ ‡ç­¾æ—¶äº¤å‰ç†µå…¬å¼ï¼ˆçœŸå®æ ‡ç­¾yæ€»ä¸º1ï¼‰
$$
H\left(\boldsymbol y^{(i)}, \boldsymbol {\hat y}^{(i)}\right ) = -\sum_{j=1}^q y_j^{(i)} \log \hat y_j^{(i)}
$$

```python
def loss(y_hat,y):
    return -torch.log(y_hat.gather(1,y.view(-1,1)))
    # torch.gatheræŒ‰ç´¢å¼•å–æ•°
    # å¦‚æ ‡ç­¾ä¸ºy=[2,0]ï¼Œå¯¹åº”çœŸå®æ¦‚ç‡ä¸º[0,0,1,...][1,0,0,...]ï¼Œåˆ™ä»y_hatä¸­å–y.viewï¼ˆå°†yå€’ç½®ï¼‰çš„æ•°å‚ä¸è®¡ç®—ï¼Œå³ç¬¬ä¸€è¡Œç¬¬2ä¸ªï¼Œç¬¬äºŒè¡Œç¬¬0ä¸ª...
```

`torch.gather`æ˜¯æŒ‰ç´¢å¼•å–æ•°

å¦‚æ ‡ç­¾ä¸ºy=[2,0]ï¼Œå¯¹åº”çœŸå®æ¦‚ç‡ä¸º[0,0,1,...]ï¼Œ[1,0,0,...]ï¼Œåˆ™ä»y_hatä¸­å–y.viewï¼ˆå°†yå€’ç½®ï¼‰çš„æ•°å‚ä¸è®¡ç®—ï¼Œå³ç¬¬ä¸€è¡Œç¬¬2ä¸ªï¼Œç¬¬äºŒè¡Œç¬¬0ä¸ª...

##### å®šä¹‰ä¼˜åŒ–å™¨

```python
def sgd(params, lr, batch_size):
    for param in params:
        param.data -= lr * param.grad / batch_size 
```

##### è®¡ç®—å‡†ç¡®ç‡

```python
def test_accuracy(data_iter,net):
    acc_sum, n=0.0, 0
    for X,y in data_iter:
        acc_sum+=(net(X).argmax(dim=1)==y).float().sum().item()  #æ³¨æ„è¿™é‡Œæ˜¯sumä¸æ˜¯mean ä¹‹åä¼šÃ·n
        n+=y.shape[0]
    return acc_sum / n
```

* argmax(dim=1)å–æ¯è¡Œæœ€å¤§å…ƒç´ ä¸”è¾“å‡ºä¸yå½¢çŠ¶ç›¸åŒ

* .float()å°†tensorè½¬æ¢ä¸ºæµ®ç‚¹å‹(çœŸ1å‡0)

* .mean()æ±‚å¹³å‡ 

* .item()æ±‚åªæœ‰ä¸€ä¸ªå…ƒç´ çš„å¼ é‡å†…çš„å…ƒç´ å€¼

##### è®­ç»ƒæ¨¡å‹

==å…ˆå°†ä¼˜åŒ–å™¨æ¢¯åº¦æ¸…é›¶ï¼Œå†å°†losså‡½æ•°åå‘ä¼ æ’­ï¼Œå†è¿­ä»£ä¼˜åŒ–å™¨==

>gradåœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­æ˜¯ç´¯åŠ çš„(accumulated)ï¼Œè¿™æ„å‘³ç€æ¯ä¸€æ¬¡è¿è¡Œåå‘ä¼ æ’­ï¼Œæ¢¯åº¦éƒ½ä¼šç´¯åŠ ä¹‹å‰çš„æ¢¯åº¦ï¼Œæ‰€ä»¥ä¸€èˆ¬åœ¨åå‘ä¼ æ’­ä¹‹å‰éœ€æŠŠæ¢¯åº¦æ¸…é›¶ã€‚

==åœ¨æ‰‹å†™å®ç°è¿‡ç¨‹ä¸­ï¼Œéœ€è¦æ³¨æ„æ¢¯åº¦æ˜¯å¦ä¸ºNoneï¼Œæ¢¯åº¦éœ€è¦åœ¨ç¬¬ä¸€æ¬¡è®¡ç®—åå˜ä¸ºå¼ é‡==

>**Tensor.grad**   Pytorch Docs
>
>This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. The attribute will then contain the gradients computed and future calls to backward() will accumulate (add) gradients into it.

```python
num_epochs=5
lr=0.1

def train(net,train_iter,test_iter,loss,num_epochs,batch_size,params,lr):

    for epoch in range(num_epochs):
        train_l_sum,train_acc_sum,n=0.0,0.0,0
        for X,y in train_iter:
            y_hat=net(X)
            l=loss(y_hat,y).sum()

            # wå’Œbæ¢¯åº¦æ¸…é›¶(æ£€æµ‹æ˜¯å¦ä¸ºNone)
            for param in params:
                if param.grad is not None:
                    param.grad.data.zero_()

            # è®¡ç®—losså‡½æ•°æ¢¯åº¦ï¼Œåå‘ä¼ æ’­
            l.backward()
            
            # æ¢¯åº¦ä¸‹é™
            sgd(params,lr,batch_size) 
               
            # losså’Œç²¾ç¡®åº¦åŠ å’Œ
            train_l_sum+=l.item()
            train_acc_sum+=(y_hat.argmax(dim=1)==y).sum().item()
            n+=y.shape[0]
        test_acc=test_accuracy(test_iter,net)
        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f' 
            % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))
```

```python
train (net, train_iter, test_iter, loss, num_epochs, batch_size, [w, b], lr)
```

##### é¢„æµ‹

è°ƒç”¨APIæ˜¾ç¤ºå›¾ç‰‡ã€çœŸå®ç»“æœå’Œé¢„æµ‹ç»“æœ

```python
from IPython import display
def use_svg_display():
    """Use svg format to display plot in jupyter"""
    display.set_matplotlib_formats('svg')

def get_fashion_mnist_labels(labels):
    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',
                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']
    return [text_labels[int(i)] for i in labels]

def show_fashion_mnist(images, labels):
    use_svg_display()
    # è¿™é‡Œçš„_è¡¨ç¤ºæˆ‘ä»¬å¿½ç•¥ï¼ˆä¸ä½¿ç”¨ï¼‰çš„å˜é‡
    _, figs = plt.subplots(1, len(images), figsize=(12, 12))
    for f, img, lbl in zip(figs, images, labels):
        f.imshow(img.view((28, 28)).numpy())
        f.set_title(lbl)
        f.axes.get_xaxis().set_visible(False)
        f.axes.get_yaxis().set_visible(False)
```

```python
X, y = iter(test_iter).next()

true_labels = get_fashion_mnist_labels(y.numpy())
pred_labels = get_fashion_mnist_labels(net(X).argmax(dim=1).numpy())
titles = [true + '\n' + pred for true, pred in zip(true_labels, pred_labels)]

show_fashion_mnist(X[0:9], titles[0:9])
```



#### Pytorchç®€æ´å®ç°

==\4_Softmax_Regression_SLP_Pytorch.ipynb==

è¯»å–æ•°æ®ã€åˆå§‹åŒ–å‚æ•°ã€è®¡ç®—å‡†ç¡®åº¦ç­‰å’Œä¸Šè¿°æ‰‹å†™ä¸€è‡´æˆ–ç›¸ä¼¼

##### å®šä¹‰æ¨¡å‹ç½‘ç»œ ä¸ åˆå§‹åŒ–æ¨¡å‹å‚æ•°

åˆ©ç”¨pytorchå¿«é€Ÿåˆ›å»ºç½‘ç»œï¼Œç¬¬ä¸€æ­¥å°†è¾“å…¥(batch_size,1,28,28)è½¬æ¢ä¸º(batch_size,784)

```python
class FlattenLayer(nn.Module):
    def __init__(self):
        super(FlattenLayer,self).__init__()
    def forward(self,x):
        return x.view(x.shape[0],-1)
```

```python
num_inputs=784
num_outputs=10

net=nn.Sequential(
    FlattenLayer(),
    nn.Linear(num_inputs, num_outputs)
)

init.normal_(net[1].weight, mean=0, std=0.01)
init.constant_(net[1].bias, val=0) 

# net = nn.Sequential(
#     OrderedDict([
#         ('flatten', FlattenLayer()),
#         ('linear', nn.Linear(num_inputs, num_outputs))
#     ])
# )

# init.normal_(net.linear.weight, mean=0, std=0.01)
# init.constant_(net.linear.bias, val=0) 
```

##### softmaxå®ç° ä¸ äº¤å‰ç†µæŸå¤±å‡½æ•°

==åˆ†å¼€å®šä¹‰softmaxè¿ç®—å’Œäº¤å‰ç†µæŸå¤±å‡½æ•°å¯èƒ½é€ æˆæ•°å€¼ä¸ç¨³å®šï¼ˆï¼Ÿï¼‰==ï¼Œpytorchçš„äº¤å‰ç†µå‡½æ•°æ•´åˆäº†softmaxå‡½æ•°ï¼Œæä¾›æ›´å¥½çš„æ•°å€¼ç¨³å®šæ€§ã€‚

```python
loss = nn.CrossEntropyLoss()
```

##### å®šä¹‰ä¼˜åŒ–ç®—æ³•

```python
optimizer = torch.optim.SGD(net.parameters(), lr=0.1)
```

##### è®­ç»ƒæ¨¡å‹

æ¢¯åº¦æ¸…é›¶å’Œæ¢¯åº¦ä¸‹é™ä½¿ç”¨å†…ç½®å‡½æ•°`optimizer.zero_grad()` `optimizer.step()`å®ç°ï¼Œæ¯”è¾ƒç®€æ´ã€‚

***

### å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLP)

ç¥ç»ç½‘ç»œä¸»è¦ä¸ºå¤šå±‚ç¥ç»ç½‘ç»œï¼Œå¤šå±‚ç¥ç»ç½‘ç»œçš„æœ€åŸºæœ¬æ¨¡å‹å³å¤šå±‚æ„ŸçŸ¥æœºã€‚

#### éšè—å±‚

å¤šå±‚æ„ŸçŸ¥æœºç›¸è¾ƒäºä¹‹å‰çº¿æ€§å›å½’ã€softmaxå›å½’ï¼Œåœ¨è¾“å…¥å±‚å’Œè¾“å‡ºå±‚ä¹‹é—´å¢åŠ äº†ä¸€åˆ°å¤šä¸ªéšè—å±‚ï¼ˆhidden layerï¼‰ã€‚å¤šå±‚æ„ŸçŸ¥æœºçš„éšè—å±‚å’Œè¾“å‡ºå±‚éƒ½æ˜¯å…¨è¿æ¥å±‚ã€‚

![3.8_mlp](figs/3.8_mlp.svg)

#### æ¿€æ´»å‡½æ•°

å…¨è¿æ¥å±‚å¦‚æœåªå¯¹æ•°æ®è¿›è¡Œçº¿æ€§çš„ä»¿å°„å˜æ¢ï¼ˆçŸ©é˜µçº¿æ€§è¿ç®—ï¼‰ï¼Œå¯ä»¥è¯æ˜å¤šæ¬¡çº¿æ€§å˜æ¢ä»ç„¶ç­‰åŒäºä¸€ä¸ªçº¿æ€§å˜æ¢ã€‚ä¸ºæ­¤éœ€è¦å¼•å…¥éçº¿æ€§å˜æ¢ã€‚è¿™ä¸ªéçº¿æ€§å‡½æ•°ç§°ä¸ºæ¿€æ´»å‡½æ•°ï¼ˆactivation functionï¼‰ï¼Œå¸¸è§çš„æ¿€æ´»å‡½æ•°æœ‰ï¼š

##### Sigmoidå‡½æ•°

æ›¾ç»æœ€ä¸»æµçš„æ¿€æ´»å‡½æ•°ï¼Œå¯ä»¥å°†å…ƒç´ çš„å€¼å˜æ¢åˆ°0-1ä¹‹é—´ã€‚
$$
\text{sigmoid}(x) = \frac{1}{1 + \exp(-x)}.
$$
<img src="figs/3.8_sigmoid.png" alt="3.8_sigmoid" style="zoom:50%;" />

[^]: sigmoidå‡½æ•°å›¾åƒ

sigmoidçš„å¯¼æ•°åœ¨è¾“å…¥ä¸º0æ—¶æœ€å¤§ï¼Œæ­¤å¤„æ¥è¿‘çº¿æ€§å˜æ¢ï¼Œä¸‹å›¾ä¸ºsigmoidçš„å¯¼æ•°å›¾åƒ

<img src="figs/3.8_sigmoid_grad.png" alt="3.8_sigmoid_grad" style="zoom:50%;" />

[^]: sigmoidå‡½æ•°å¯¼æ•°å›¾åƒ

ä¼˜ç‚¹ï¼š

* å¤„å¤„è¿ç»­ï¼Œä¾¿äºæ±‚å¯¼
* å°†å‡½æ•°å€¼èŒƒå›´å‹ç¼©åˆ°[0,1]ï¼Œä¸”å¹…åº¦ä¸å˜ã€‚ï¼ˆåœ¨å¾ªç¯ç¥ç»ç½‘ç»œæ—¶ä¼šåˆ©ç”¨è¿™ä¸€ç‰¹æ€§æ§åˆ¶ä¿¡æ¯åœ¨ç¥ç»ç½‘ç»œä¸­çš„æµåŠ¨ï¼‰
* å¯ç”¨äºç½‘ç»œè¾“å‡ºå±‚åšäºŒåˆ†ç±»ï¼Œè¾ƒå°‘ç”¨äºéšè—å±‚

ç¼ºç‚¹ï¼š

* å¹‚å‡½æ•°è®¡ç®—é‡å¤§
* å½“zå¾ˆå¤§æˆ–å¾ˆå°æ—¶å¯¼æ•°æ¥è¿‘äº0ï¼Œåå‘ä¼ æ’­æ—¶æ›´æ–°æ…¢
* è¾“å‡ºä¸ä»¥0ä¸ºå‡å€¼ï¼Œè‹¥åå±‚ç¥ç»å…ƒè¾“å…¥æ˜¯é0å‡å€¼ï¼Œè®­ç»ƒæ—¶wå®¹æ˜“å¾€ä¸€ä¸ªæ–¹å‘æ›´æ–°
* æ·±åº¦ç¥ç»ç½‘ç»œä¸­å®¹æ˜“å‡ºç°**æ¢¯åº¦æ¶ˆå¤±**ï¼š==ç”±ä¸Šå›¾å¯çŸ¥sigmoidçš„å¯¼æ•°è¾ƒå°ï¼ˆå°¤å…¶æ˜¯è¾“å…¥è¾ƒå¤§çš„æ—¶å€™ï¼‰ï¼Œå¦‚æœæ¯å±‚éšè—å±‚éƒ½ä½¿ç”¨sigmoidï¼Œå…¶æ¢¯åº¦é€šè¿‡é“¾å¼æ³•åˆ™æ—¶ä¼šæœ‰å¤šä¸ªå¾ˆå°çš„æ•°å‚ä¸è¿ç®—ï¼Œæå°çš„æ¢¯åº¦å€¼ä½¿å¾—åˆå§‹å±‚çš„æƒå€¼å’Œåç½®å‡ ä¹æ— æ³•æœ‰æ•ˆçš„æ›´æ–°ã€‚==

***

##### tanhå‡½æ•°

åŒæ›²æ­£åˆ‡å‡½æ•°å¯ä»¥å°†å…ƒç´ çš„å€¼å˜æ¢ä¸º-1åˆ°1ä¹‹é—´
$$
\text{tanh}(x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)}.
$$
å‡½æ•°å›¾åƒå’Œå¯¼æ•°å›¾åƒä¸sigmoidå½¢çŠ¶ç›¸ä¼¼ï¼Œä½†å‡½æ•°å›¾åƒå…³äºåŸç‚¹å¯¹ç§°

<img src="figs/3.8_tanh.png" alt="3.8_tanh" style="zoom:50%;" />

[^]: tanhå‡½æ•°å›¾åƒ

<img src="figs/3.8_tanh_grad.png" alt="3.8_tanh_grad" style="zoom:50%;" />

[^]: tanhå‡½æ•°å¯¼æ•°å›¾åƒ

ä¼˜ç‚¹ï¼š

* å€¼åŸŸ[-1,1]ï¼Œä»¥0ä¸ºå‡å€¼ï¼Œå®é™…åº”ç”¨ä¸­ä¼˜äºsigmoidå‡½æ•°
* åœ¨å®é™…åº”ç”¨ä¸­å‘ç°ï¼Œç‰¹å¾ç›¸å·®æ˜æ˜¾çš„æ—¶å€™æ•ˆæœæ›´å¥½ï¼ˆï¼Ÿï¼‰

ç¼ºç‚¹ï¼š

* å¹‚å‡½æ•°ç›¸å¯¹è€—æ—¶
* æ·±åº¦å­¦ä¹ ä¸­ä»ç„¶é¢ä¸´æ¢¯åº¦æ¶ˆå¤±é—®é¢˜

***

##### ReLUå‡½æ•°

ReLUï¼ˆrectified linear unitï¼‰å‡½æ•°æä¾›äº†ä¸€ä¸ªå¾ˆç®€å•çš„éçº¿æ€§å˜æ¢ï¼Œæ˜¯ç›®å‰æœ€å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°ã€‚ç»™å®šå…ƒç´ $x$ï¼Œè¯¥å‡½æ•°å®šä¹‰ä¸º

$$
\text{ReLU}(x) = \max(x, 0).
$$
<img src="figs/3.8_relu.png" alt="3.8_relu" style="zoom:50%;" />

[^]: ReLUå‡½æ•°å›¾åƒ

å¯ä»¥çœ‹å‡ºï¼ŒReLUå‡½æ•°åªä¿ç•™æ­£æ•°å…ƒç´ ï¼Œå¹¶å°†è´Ÿæ•°å…ƒç´ æ¸…é›¶ã€‚è¯¥å‡½æ•°ä¸º

ä¸¤æ®µçš„åˆ†æ®µçº¿æ€§å‡½æ•°ï¼Œåœ¨0å¤„ä¸å¯å¯¼ï¼ˆä½†å¯ä»¥å–å¯¼æ•°ä¸º0ï¼‰

<img src="figs/3.8_relu_grad.png" alt="3.8_relu_grad" style="zoom:50%;" />

[^]: ReLUå‡½æ•°å¯¼æ•°å›¾åƒ

ä¼˜ç‚¹ï¼š

* å¼¥è¡¥sigmoidå‡½æ•°å’Œtanhå‡½æ•°æ¢¯åº¦æ¶ˆå¤±é—®é¢˜
* æ±‚å¯¼ç®€å•ï¼Œè®¡ç®—è¾ƒå¿«ï¼Œåœ¨æ¢¯åº¦ä¸‹é™ä¸­æ”¶æ•›é€Ÿåº¦æ¯”tanh/sigmoidå‡½æ•°å¿«å¾ˆå¤š

ç¼ºç‚¹

* è¾“å‡ºä¸æ˜¯zero-centered
* åœ¨è´Ÿæ•°åŒºåŸŸè¢«killï¼Œè®­ç»ƒæ—¶å¯èƒ½å¯¼è‡´æœ‰äº›ç¥ç»å…ƒæ°¸è¿œä¸è¢«æ¿€æ´»ï¼Œç›¸åº”å‚æ•°ä¸å¾—åˆ°æ›´æ–°

##### Leaky ReLUå‡½æ•°

è§£å†³ä¸Šè¿°ReLUå‡½æ•°çš„ç¬¬äºŒæ¡ç¼ºç‚¹ï¼Œåœ¨ReLUçš„è´Ÿæ•°éƒ¨åˆ†è®¾å®šä¸€ä¸ªå¾ˆå°çš„æƒå€¼ï¼Œä½¿è´Ÿæ•°ä¸ä¼šè¿…é€Ÿâ€œæ­»æ‰â€

ç†è®ºä¸Šæ¥è¯´ï¼ŒLeaky ReLUå‡½æ•°æœ‰ReLUçš„ä¼˜ç‚¹ä¸”é¿å…äº†å…¶Dead ReLUé—®é¢˜ï¼Œä½†==å®é™…æ“ä½œä¸­æ²¡æœ‰å®Œå…¨è¯æ˜å…¶æ€»æ˜¯å¥½äºReLU==ã€‚

![image-20220702181642405](figs/image-20220702181642405.png)

##### æ€»ç»“

>* å¤šå±‚æ„ŸçŸ¥æœºåœ¨è¾“å‡ºå±‚ä¸è¾“å…¥å±‚ä¹‹é—´åŠ å…¥äº†ä¸€ä¸ªæˆ–å¤šä¸ªå…¨è¿æ¥éšè—å±‚ï¼Œå¹¶é€šè¿‡æ¿€æ´»å‡½æ•°å¯¹éšè—å±‚è¾“å‡ºè¿›è¡Œå˜æ¢ã€‚
>* å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°åŒ…æ‹¬ReLUå‡½æ•°ã€sigmoidå‡½æ•°å’Œtanhå‡½æ•°ã€‚
>* ä¸ä½¿ç”¨æ¿€æ´»å‡½æ•°ï¼Œå¤šå±‚ç›¸å½“äºä¸€å±‚ï¼Œä¸”æ²¡æœ‰å¯¹éçº¿æ€§å‡½æ•°çš„è¡¨è¾¾èƒ½åŠ›
>* ä½¿ç”¨éçº¿æ€§çš„æ¿€æ´»å‡½æ•°ï¼Œé€šè¿‡ç¥ç»ç½‘ç»œä¸æ–­åŠ æ·±ï¼Œç¥ç»ç½‘ç»œå°±å¯ä»¥é€¼è¿‘ä»»ä½•éçº¿æ€§å‡½æ•°ï¼Œå¯ä»¥æ„å»ºå‡ºä»»ä½•å¤æ‚å‡½æ•°ã€‚ï¼ˆéšè—å±‚é—´æ¯å±‚éœ€è¦åŠ æ¿€æ´»å‡½æ•°ï¼‰

***
#### M-Pç¥ç»å…ƒæ¦‚å¿µ

M-Pç¥ç»å…ƒæ¨¡å‹æ˜¯æ¨¡ä»¿åŠ¨ç‰©å¤§è„‘ç¥ç»å…ƒçš„æœ€æ—©ç¤ºä¾‹ã€‚å³ï¼šå…ˆçº¿æ€§è¿ç®—+å†æ¿€æ´»å‡½æ•° æ„æˆçš„åŸºæœ¬ç¥ç»å…ƒ

==ä¸‹å›¾ä¸­$$\theta$$å°±æ˜¯çº¿æ€§è¿ç®—ä¸­çš„biasï¼ˆå­˜ç–‘ï¼Ÿï¼‰==

<img src="figs/2018090813231980.jpg" alt="2018090813231980" style="zoom:40%;" />



#### æ‰‹å†™å®ç°

==\5_Softmax_Regression_MLP.ipynb==

åŸºäºä¸Šä¸€èŠ‚ Fashion-MNISTï¼Œå¼•å…¥åŒå±‚MLPï¼Œå³å®šä¹‰æ¿€æ´»å‡½æ•°å¹¶æ”¹å˜æ¨¡å‹å¦‚ä¸‹ï¼š

##### å®šä¹‰åˆå§‹åŒ–å‚æ•°

ç”±äºå¼•å…¥äº†éšè—å±‚ï¼Œåˆ™å˜ä¸ºçº¿æ€§è¿ç®—+ReLU+çº¿æ€§è¿ç®—ï¼Œæ•…éœ€è¦ä¸¤å¥—çº¿æ€§å‚æ•°

å‡è®¾ä¸­é—´éšè—å±‚æœ‰256ä¸ªèŠ‚ç‚¹

```python
num_inputs=784
num_hidden=256
num_outputs=10

w1=torch.tensor(np.random.normal(0,0.01,(num_inputs,num_hidden)),dtype=torch.float)
b1=torch.zeros(num_hidden,dtype=torch.float)
w2=torch.tensor(np.random.normal(0,0.01,(num_hidden,num_outputs)),dtype=torch.float)
b2=torch.zeros(num_outputs,dtype=torch.float)

w1.requires_grad_(requires_grad=True)
b1.requires_grad_(requires_grad=True)
w2.requires_grad_(requires_grad=True)
b2.requires_grad_(requires_grad=True)
```

##### å®šä¹‰æ¿€æ´»å‡½æ•°

æ­¤å¤„ä½¿ç”¨ReLU

```python
def relu(x):
    return torch.max(input=x,other=torch.tensor(0.0))
```

##### æ”¹å˜æ¨¡å‹ç½‘ç»œä¸ºåŒå±‚

```python
def net(x):
    H=relu(torch.mm(x.view(-1,num_inputs),w1)+b1)
    return softmax(torch.mm(H,w2)+b2)
```



#### Pytorchç®€æ´å®ç°

==\6_Softmax_Regression_MLP_Pytorch.ipynb==

##### æ”¹å˜æ¨¡å‹ç½‘ç»œä¸ºåŒå±‚

å°†ä¸ŠèŠ‚å•å±‚æ¨¡å‹ç½‘ç»œpytorchç®€æ´å®ç°ä¸­çš„ç½‘ç»œä¿®æ”¹å¦‚ä¸‹ï¼š

```python
class FlattenLayer(nn.Module): #å±•å¹³è¾“å…¥ï¼Œä½¿ä¹‹å¯ä»¥å‚ä¸çŸ©é˜µè¿ç®—
    def __init__(self):
        super(FlattenLayer,self).__init__()
    def forward(self,x):
        return x.view(x.shape[0],-1)
```

```python
num_inputs,num_hidden,num_outputs=784,256,10

net=nn.Sequential(
    FlattenLayer(),
    nn.Linear(num_inputs, num_hidden),
    nn.ReLU(),  # pytorchæä¾›äº†å†…ç½®çš„ReLUå‡½æ•°ä»¥ä¾›ç›´æ¥è°ƒç”¨
    nn.Linear(num_hidden, num_outputs),
)

for param in net.parameters():
    init.normal_(param,mean=0,std=0.01)
```

==åˆ«å¿˜äº†åˆå§‹åŒ–å‚æ•°ï¼ï¼ˆè™½ç„¶æ²¡æœ‰åˆå§‹åŒ–å¥½åƒä¹Ÿå¯ä»¥ç‚¼å‡ºæ¥ğŸ¤¡ï¼Œæ˜¯å› ä¸ºpytorchçš„Linearå·²ç»åˆå§‹åŒ–äº†ï¼‰==



### æ¨¡å‹é€‰æ‹©ä¸æ¬ /è¿‡æ‹Ÿåˆ

#### è¯¯å·®

* è®­ç»ƒè¯¯å·®ï¼ˆtraining errorï¼‰ï¼šæ¨¡å‹åœ¨è®­ç»ƒæ•°æ®é›†ä¸Šè¡¨ç°å‡ºçš„è¯¯å·®

* æ³›åŒ–è¯¯å·®ï¼ˆgeneralization errorï¼‰ï¼šæ¨¡å‹åœ¨ä»»ä½•ä¸€ä¸ªæµ‹è¯•æ•°æ®æ ·æœ¬ä¸Šè¡¨ç°çš„è¯¯å·®ï¼Œé€šå¸¸ç”¨æ¨¡å‹åœ¨æµ‹è¯•æ•°æ®é›†ä¸Šçš„è¯¯å·®æ¥è¿‘ä¼¼ã€‚

  å‚æ•°é€‰æ‹©ä¾æ®æœ€å°åŒ–è®­ç»ƒè¯¯å·®ï¼Œè®­ç»ƒè¯¯å·®çš„æœŸæœ›å°äºæˆ–ç­‰äºæ³›åŒ–è¯¯å·®ã€‚ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œç”±è®­ç»ƒæ•°æ®é›†å­¦åˆ°çš„æ¨¡å‹å‚æ•°ä¼šä½¿æ¨¡å‹åœ¨è®­ç»ƒæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºæˆ–ç­‰äºåœ¨æµ‹è¯•æ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚

  é™ä½è®­ç»ƒè¯¯å·®ä¸æ„å‘³ç€æ³›åŒ–è¯¯å·®ä¸€å®šä¼šé™ä½ï¼Œä½†æœºå™¨å­¦ä¹ åº”ä»¥é™ä½æ³›åŒ–è¯¯å·®ä¸ºç›®æ ‡ã€‚

#### æ¨¡å‹é€‰æ‹©

##### éªŒè¯æ•°æ®é›†

æµ‹è¯•é›†åœ¨ä¸¥æ ¼æ„ä¹‰ä¸Šæ¥è®²ï¼Œåªèƒ½åœ¨æ‰€æœ‰è¶…å‚æ•°å’Œæ¨¡å‹å‚æ•°é€‰å®šåä½¿ç”¨ä¸€æ¬¡ï¼Œä¸å¯ä»¥æ ¹æ®æµ‹è¯•æ•°æ®é›†æ›´æ”¹æ¨¡å‹ã€‚ä½†ç”±äºæ— æ³•ä»è®­ç»ƒè¯¯å·®ä¼°è®¡æ³›åŒ–è¯¯å·®ï¼Œä¹Ÿä¸åº”åªä¾èµ–è®­ç»ƒæ•°æ®é€‰æ‹©æ¨¡å‹ã€‚ç”±æ­¤ï¼Œå¯ä»¥é¢„ç•™ä¸€éƒ¨åˆ†åœ¨è®­ç»ƒæ•°æ®é›†å’Œæµ‹è¯•æ•°æ®é›†ä»¥å¤–çš„æ•°æ®æ¥è¿›è¡Œæ¨¡å‹é€‰æ‹©ï¼Œè¯¥éƒ¨åˆ†æ•°æ®ç§°ä¸ºéªŒè¯æ•°æ®é›†ã€‚

##### $K$æŠ˜äº¤å‰éªŒè¯

ç”±äºéªŒè¯æ•°æ®é›†ä¸å‚ä¸æ¨¡å‹è®­ç»ƒï¼Œå½“è®­ç»ƒæ•°æ®ä¸å¤Ÿç”¨æ—¶ï¼Œé¢„ç•™å¤§é‡éªŒè¯æ•°æ®è¿‡äºå¥¢ä¾ˆã€‚æ­¤æ—¶å¯ä»¥ä½¿ç”¨$K$æŠ˜äº¤å‰éªŒè¯ï¼ˆ$K$-fold cross-validationï¼‰ï¼šæŠŠåŸå§‹è®­ç»ƒæ•°æ®é›†åˆ†å‰²æˆ$K$ä¸ªä¸é‡åˆçš„å­æ•°æ®é›†ã€‚æ¯æ¬¡é€‰ä¸€ä¸ªå­æ•°æ®é›†éªŒè¯ï¼Œå…¶ä½™$K-1$ä¸ªç”¨äºè®­ç»ƒï¼Œåˆ†åˆ«æ±‚è®­ç»ƒè¯¯å·®å’ŒéªŒè¯è¯¯å·®ã€‚æœ€åå¯¹ä¸¤è€…æ±‚å¹³å‡ã€‚

#### æ¨¡å‹å¤æ‚åº¦

ä»¥å¤šé¡¹å¼å‡½æ•°æ‹Ÿåˆä¸ºä¾‹ï¼Œå¤šé¡¹å¼å‡½æ•°æ‹Ÿåˆçš„ç›®æ ‡æ˜¯æ‰¾ä¸€ä¸ª$K$é˜¶å¤šé¡¹å¼å‡½æ•°
$$
\hat{y} = b + \sum_{k=1}^K x^k w_k
$$
é«˜é˜¶å¤šé¡¹å¼æ¨¡å‹å‚æ•°æ›´å¤šï¼Œå¤æ‚åº¦æ›´é«˜ã€‚

#### æ¬ æ‹Ÿåˆä¸è¿‡æ‹Ÿåˆ

æ¨¡å‹å¤æ‚åº¦è¿‡é«˜æ˜“å‡ºç°è¿‡æ‹Ÿåˆï¼Œå¤æ‚åº¦è¿‡ä½æ˜“å‡ºç°æ¬ æ‹Ÿåˆã€‚åº”å¯¹æ¬ æ‹Ÿåˆå’Œè¿‡æ‹Ÿåˆçš„ä¸€ä¸ªåŠæ³•æ˜¯é’ˆå¯¹æ•°æ®é›†é€‰æ‹©åˆé€‚å¤æ‚åº¦çš„æ¨¡å‹ã€‚

å½±å“æ¬ æ‹Ÿåˆå’Œè¿‡æ‹Ÿåˆå¦ä¸€ä¸ªé‡è¦å› ç´ æ˜¯è®­ç»ƒæ•°æ®é›†å¤§å°â€”â€”å¦‚æœè®­ç»ƒæ•°æ®é›†ä¸­æ ·æœ¬æ•°è¿‡å°‘ï¼Œç‰¹åˆ«æ˜¯æ¯”æ¨¡å‹å‚æ•°æ•°é‡ï¼ˆæŒ‰å…ƒç´ è®¡ï¼‰æ›´å°‘æ—¶ï¼Œè¿‡æ‹Ÿåˆæ›´å®¹æ˜“å‘ç”Ÿã€‚

æ³¨ï¼šæ³›åŒ–è¯¯å·®ä¸ä¼šéšè®­ç»ƒæ•°æ®é›†é‡Œæ ·æœ¬å¢åŠ è€Œå¢å¤§ï¼Œæ‰€ä»¥é€šå¸¸å¸Œæœ›è®­ç»ƒæ•°æ®é›†å¤§ä¸€äº›ã€‚

<img src="figs/image-20220703211612466.png" alt="image-20220703211612466" style="zoom:80%;" />


#### å¤šé¡¹å¼æ‹Ÿåˆå®éªŒ

==\7_Polynormal_Fitting.ipynb==

é€šè¿‡ä¸€å…ƒä¸‰æ¬¡å¤šé¡¹å¼å‡½æ•°çš„æ‹Ÿåˆå®éªŒå¯ä»¥å¾—åˆ°è¿‡æ‹Ÿåˆå’Œæ¬ æ‹Ÿåˆçš„ç»“æœã€‚

* è®­ç»ƒé›†å’Œæµ‹è¯•é›†1ï¼š1æ—¶æ­£å¸¸æ‹Ÿåˆ
* è®­ç»ƒé›†è¿‡å°‘æ—¶è¿‡æ‹Ÿåˆ
* ä½¿ç”¨çº¿æ€§æ¨¡å‹æ‹Ÿåˆæ—¶æ¬ æ‹Ÿåˆï¼ˆæ¨¡å‹å¤æ‚åº¦è¿‡ä½ï¼‰

### æƒé‡è¡°å‡ä¸ä¸¢å¼ƒ

==ç¥ç»ç½‘ç»œå¾—åˆ°ç›¸åŒçš„losså¯ä»¥æœ‰å¾ˆå¤šç»„è§£ï¼ˆw,bï¼‰ï¼Œè¿™ä¸è®­ç»ƒå‚æ•°çš„åˆå§‹å€¼æœ‰å…³ã€‚==

ä¸ºäº†åº”å¯¹è¿‡æ‹Ÿåˆï¼Œå¯ä»¥å¢å¤§æ•°æ®é›†ä½†ä»£ä»·é«˜æ˜‚ã€‚åº”å¯¹è¿‡æ‹Ÿåˆè¿˜æœ‰æƒé‡è¡°å‡æ³•ï¼ˆweight decayï¼‰ï¼Œå³æ­£åˆ™åŒ–ï¼Œé™åˆ¶å‚æ•°è¿‡å¤šæˆ–è€…è¿‡å¤§ï¼Œé¿å…æ¨¡å‹è¿‡äºå¤æ‚ï¼›ä»¥åŠä¸¢å¼ƒæ³•ã€‚

å‡å°‘å‚æ•°ä¸ªæ•°è¾ƒéš¾æŠŠæ¡ï¼Œå¯ä»¥è€ƒè™‘é™å®šå‚æ•°å¤§å°ï¼Œå¦‚è§„å®šæ¡ä»¶$\sum_{j} w_j<C$

$L_1$ä¸$L_2$æ­£åˆ™åŒ–é€šè¿‡ä¸ºæŸå¤±å‡½æ•°å¢åŠ æƒ©ç½šé¡¹ï¼Œä»è€Œä½¿å­¦å‡ºçš„æ¨¡å‹å‚æ•°å€¼è¾ƒå°ã€‚

#### $L_1$æ­£åˆ™åŒ–

$L_1$æ­£åˆ™åŒ–å…¬å¼è¾ƒä¸ºç®€å•ï¼Œç›´æ¥åœ¨åŸæ¥æŸå¤±å‡½æ•°ååŠ ä¸Šæƒé‡å‚æ•°ç»å¯¹å€¼ã€‚ä»¥çº¿æ€§å›å½’æŸå¤±å‡½æ•°ä¸ºä¾‹ï¼Œæ–°çš„æŸå¤±å‡½æ•°ä¸º
$$
\ell(w, b) + \frac{\lambda}{n} \sum_{} |w_j|
$$

#### $L_2$æ­£åˆ™åŒ–

$L_2$èŒƒæ•°æƒ©ç½šé¡¹æŒ‡çš„æ˜¯æ¨¡å‹æƒé‡å‚æ•°æ¯ä¸ªå…ƒç´ çš„å¹³æ–¹å’Œä¸ä¸€ä¸ªæ­£å¸¸æ•°çš„ä¹˜ç§¯ã€‚ä»¥çº¿æ€§å›å½’æŸå¤±å‡½æ•°ä¸ºä¾‹ï¼Œæ–°çš„æŸå¤±å‡½æ•°ä¸º

$$
\ell(w, b) + \frac{\lambda}{2n} \|\boldsymbol{w}\|^2
$$

å…¶ä¸­$n$ä¸ºæ ·æœ¬æ•°ã€‚å‘é‡$\boldsymbol{w} = [w_1, w_2]$ä¸ºæƒé‡å‚æ•°ã€‚è¶…å‚æ•°$\lambdaï¼0$ã€‚å½“$\lambda$è¾ƒå¤§æ—¶ï¼Œæƒ©ç½šé¡¹åœ¨æŸå¤±å‡½æ•°ä¸­æ¯”é‡è¾ƒå¤§ï¼Œè¿™é€šå¸¸ä¼šä½¿å­¦åˆ°çš„æƒé‡å‚æ•°å…ƒç´ è¾ƒæ¥è¿‘äº0ã€‚

ä¸Šå¼ä¸­$L_2$èŒƒæ•°å¹³æ–¹$\|\boldsymbol{w}\|^2$å±•å¼€åå¾—åˆ°$\sum_{j}w_j^2$ï¼Œæ±‚æ¢¯åº¦æ—¶å˜ä¸º$w_i$ã€‚åŠ å…¥$L_2$èŒƒæ•°æƒ©ç½šé¡¹åï¼Œåœ¨å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™ä¸­ï¼Œæˆ‘ä»¬å°†çº¿æ€§å›å½’ä¸€èŠ‚ä¸­æƒé‡$w_1$å’Œ$w_2$çš„è¿­ä»£æ–¹å¼æ›´æ”¹ä¸º

$$
\begin{aligned}
w_1 &\leftarrow \left(1- \frac{\eta\lambda}{|\mathcal{B}|} \right)w_1 -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_1^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right),\\
w_2 &\leftarrow \left(1- \frac{\eta\lambda}{|\mathcal{B}|} \right)w_2 -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_2^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right).
\end{aligned}
$$

è¿™é€šè¿‡æƒ©ç½šç»å¯¹å€¼è¾ƒå¤§çš„æ¨¡å‹å‚æ•°ä¸ºéœ€è¦å­¦ä¹ çš„æ¨¡å‹å¢åŠ é™åˆ¶ï¼Œå¯èƒ½å¯¹è¿‡æ‹Ÿåˆæœ‰æ•ˆã€‚

##### æ‰‹å†™å®ç°

```python
def l2_penalty(w):
    return (w**2).sum() / 2
```

```python
l = (loss(net(X, w, b), y) + lambd * l2_penalty(w)).sum()
# æ³¨æ„.sum()å°†losså€¼è½¬ä¸ºæ ‡é‡ï¼Œä¾¿äºbackwardè®¡ç®—
```

##### Pytorchç®€æ´å®ç°

æ„é€ `optimizer`æ—¶å¯ä»¥æŒ‡å®š`weight_decay`å‚æ•°è¿›è¡Œæƒé‡è¡°å‡ã€‚é»˜è®¤ä¸‹ï¼ŒPyTorchä¼šå¯¹æƒé‡å’Œåå·®åŒæ—¶è¡°å‡ã€‚æˆ‘ä»¬å¯ä»¥åˆ†åˆ«å¯¹æƒé‡å’Œåå·®æ„é€ ä¼˜åŒ–å™¨å®ä¾‹ï¼Œä»è€Œåªå¯¹æƒé‡è¡°å‡ã€‚

```python
optimizer_w = torch.optim.SGD(params=[net.weight], lr=lr, weight_decay=wd) # å¯¹æƒé‡å‚æ•°è¡°å‡
    optimizer_b = torch.optim.SGD(params=[net.bias], lr=lr)  # ä¸å¯¹åå·®å‚æ•°è¡°å‡
    
    #ä¸åˆ†åˆ«æŒ‡å®šæ—¶,SGDçš„ç¬¬ä¸€ä¸ªå‚æ•°ä¸ºnet.parameters
```

![å·¦ä¸ºL2æ­£åˆ™åŒ–ï¼Œå³ä¸ºL1æ­£åˆ™åŒ–](https://img-blog.csdn.net/20180621090405436?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3JlZF9zdG9uZTE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

#### ä¸¢å¼ƒæ³•

åœ¨å¤šå±‚ç¥ç»ç½‘ç»œä¸­ï¼Œå¯¹éšè—å±‚çš„éšè—å•å…ƒä»¥ä¸€å®šæ¦‚ç‡ä¸¢å¼ƒã€‚**ä¸¢å¼ƒæ³•ä¸æ”¹å˜å…¶è¾“å…¥çš„æœŸæœ›å€¼**

ä¸¢å¼ƒæ¦‚ç‡ä¸º$p$ï¼Œé‚£ä¹ˆæœ‰$p$çš„æ¦‚ç‡$h_i$ä¼šè¢«æ¸…é›¶ï¼Œæœ‰$1-p$çš„æ¦‚ç‡$h_i$ä¼šé™¤ä»¥$1-p$åšæ‹‰ä¼¸ã€‚ä¸¢å¼ƒæ¦‚ç‡æ˜¯ä¸¢å¼ƒæ³•çš„è¶…å‚æ•°ã€‚

##### æ‰‹å†™å®ç°

==\8_MLP_Dropout.ipynb==

###### å®ç°Dropoutå‡½æ•°

==æ³¨æ„ï¼Œtensorç›¸ä¹˜ä¸æ˜¯çŸ©é˜µç›¸ä¹˜ï¼ˆçŸ©é˜µä¹˜æ³•æ˜¯torch.mmï¼‰ï¼Œç›¸åŒshapeçš„ä¸¤ä¸ªtensorç›¸ä¹˜æ˜¯å°†ä¸¤ä¸ªtensorä¸­æ¯ä¸€ä½å¯¹åº”ç›¸ä¹˜ã€‚==

å› æ­¤å¯ä»¥åˆ¶ä½œä¸€ä¸ª**ç”±0å’Œ1éšæœºç»„æˆçš„mask-tensorä¸xç›¸ä¹˜**ï¼Œä»¥åšåˆ°éšæœºdropout

```python
def dropout(x,drop_prob):
    x=x.float() # å°†xå˜ä¸ºfloatç±»å‹ï¼Œå‡†å¤‡åç»­å¯èƒ½çš„æ‹‰ä¼¸
    assert 0<=drop_prob<=1 # assertè¯­å¥ï¼Œè‹¥æ»¡è¶³æ¡ä»¶ç»§ç»­æ‰§è¡Œ
    keep_prob=1-drop_prob
    if not keep_prob:
        return torch.zeros(x.shape) # æˆ–zeros_like(x)
    else:
        mask=(torch.randn(x.shape)<keep_prob).float()
        # æŒ‰ç…§æ¡ä»¶éšæœºç”Ÿæˆmask-tensor
        return mask*x/keep_prob 
```

###### å®šä¹‰æ¨¡å‹ç½‘ç»œ

```python
drop_prob1,drop_prob2=0.2,0.4

def net(x,is_training=True):
    H1=relu(torch.mm(x.view(-1,num_inputs),w1)+b1)
    if is_training:
        H1=dropout(H1,drop_prob1)
    H2=relu(torch.mm(H1,w2)+b2)
    if is_training:
        H2=dropout(H2,drop_prob2)
    return softmax(torch.mm(H2,w3)+b3)

# è®­ç»ƒæ—¶dropoutï¼Œæµ‹è¯•æ—¶ä¸dropout
```

æ³¨æ„åœ¨è®¡ç®—æµ‹è¯•å‡†ç¡®åº¦æ—¶éœ€è°ƒç”¨ `net(x,False)`

##### Pytorchç®€æ´å®ç°

==\9_MLP_Dropout_Pytorch.ipynb==

Pytorchæä¾›äº†dropoutå±‚è¿›è¡Œéšæœºä¸¢å¼ƒï¼Œå¹¶ä¸”åœ¨è®­ç»ƒæ—¶å¯ç”¨ï¼ŒéªŒè¯æ—¶ä¸å¯ç”¨

å…³äºpytorchå¦‚ä½•è¯†åˆ«æ˜¯å¦æ˜¯è®­ç»ƒé›†ï¼šåœ¨æœ¬æœºçš„torchvisionè¯»å–è¯¥æ•°æ®åº“æ—¶ï¼Œå»ºç«‹datasetçš„å‡½æ•°æ ‡æ³¨æœ‰boolå‹å‚æ•°train

æ‰€ä»¥å½“train_iterä¼ å…¥æ•°æ®æ—¶åˆ¤æ–­ä¸ºè®­ç»ƒæ¨¡å¼ï¼Œtest_iterä¼ å…¥æ•°æ®æ—¶åˆ¤æ–­ä¸ºæµ‹è¯•æ¨¡å¼ã€‚

```python
class FlattenLayer(nn.Module):
    def __init__(self):
        super(FlattenLayer,self).__init__()
    def forward(self,x):
        return x.view(x.shape[0],-1)

num_inputs,num_hidden1,num_hidden2,num_outputs=784,512,256,10
drop_prob1,drop_prob2=0.2,0.5

net=nn.Sequential(
    FlattenLayer(),
    nn.Linear(num_inputs, num_hidden1),
    nn.ReLU(),
    nn.Dropout(drop_prob1),
    nn.Linear(num_hidden1, num_hidden2),
    nn.ReLU(),
    nn.Dropout(drop_prob2),
    nn.Linear(num_hidden2, num_outputs)
)

for param in net.parameters():
    init.normal_(param,mean=0,std=0.01)
```

### æ•°å€¼ç¨³å®šæ€§ä¸å‚æ•°åˆå§‹åŒ–

æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­æœ‰å…³æ•°å€¼ç¨³å®šæ€§çš„å…¸å‹é—®é¢˜å³è¡°å‡ï¼ˆvanishingï¼‰å’Œçˆ†ç‚¸ï¼ˆexplosionï¼‰

#### è¡°å‡å’Œçˆ†ç‚¸

å½“ç¥ç»ç½‘ç»œå±‚æ•°è¾ƒå¤šæ—¶ï¼Œæ¨¡å‹çš„ç¨³å®šæ€§å®¹æ˜“å˜å·®ã€‚å‡è®¾è¾“å…¥å’Œæ‰€æœ‰å±‚çš„æƒé‡å‚æ•°éƒ½æ˜¯æ ‡é‡ï¼Œå¦‚æƒé‡å°äº1æ—¶ï¼Œç»å†äº†å¤šå±‚çš„çº¿æ€§ä¹˜æ³•åå°†ä¼šè¶‹è¿‘äº0ï¼ˆè¡°å‡ï¼‰ã€‚å¤§äº1åˆ™ä¼šåœ¨å¤šå±‚çº¿æ€§ä¹˜æ³•åæ•°å€¼è†¨èƒ€å¾—æå¤§ï¼ˆçˆ†ç‚¸ï¼‰

å±‚æ•°è¾ƒå¤šæ—¶ï¼Œä¸ä»…è¾ƒæ·±å±‚è¾“å‡ºä¼šè¡°å‡/çˆ†ç‚¸ï¼Œæ¢¯åº¦çš„è®¡ç®—ä¹Ÿå®¹æ˜“å‡ºç°è¡°å‡/çˆ†ç‚¸ã€‚

#### æ¨¡å‹å‚æ•°åˆå§‹åŒ–

ç¥ç»ç½‘ç»œé€šå¸¸éœ€è¦éšæœºåˆå§‹åŒ–æ¨¡å‹å‚æ•°ã€‚å‡è®¾å¤šå±‚ç½‘ç»œä»…æœ‰ä¸€ä¸ªè¾“å‡ºå•å…ƒï¼Œä¸”éšè—å±‚ä½¿ç”¨æ¿€æ´»å‡½æ•°ç›¸åŒã€‚åˆ™è‹¥æ¯ä¸ªéšè—å•å…ƒçš„å‚æ•°åˆå§‹åŒ–ä¸ºç›¸åŒçš„å€¼ï¼Œåˆ™æ­£å‘ä¼ æ’­æ—¶æ¯ä¸ªéšè—å•å…ƒå°†æ ¹æ®ç›¸åŒçš„è¾“å…¥è®¡ç®—å‡ºç›¸åŒçš„å€¼ï¼Œåå‘ä¼ æ’­æ—¶æ¯ä¸ªéšè—å•å…ƒå‚æ•°æ¢¯åº¦å€¼ç›¸ç­‰ï¼Œå› è€Œè¿­ä»£ä¼˜åŒ–åå‚æ•°æ¢¯åº¦å€¼ä¾ç„¶ç›¸ç­‰ã€‚å¤šä¸ªéšè—å•å…ƒæœ¬è´¨ä¸Šä¸1ä¸ªéšè—å•å…ƒç›¸åŒã€‚å› æ­¤æƒé‡å‚æ•°éœ€è¦éšæœºåˆå§‹åŒ–

##### pytorchçš„éšæœºåˆå§‹åŒ–

* å¯ä»¥ä½¿ç”¨`torch.nn.init`ä¸­çš„å‡½æ•°å¯¹å‚æ•°éšæœºåˆå§‹åŒ–ï¼ˆå¦‚`torch.nn.init.normal_()`æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–ã€‚
* ä¸Šè¿°å„èŠ‚â€œç®€æ´å®ç°â€ä¸­`nn.Module`ä¸­çš„å±‚æ¨¡å—éƒ½é»˜è®¤å¯¹å‚æ•°é‡‡å–äº†åˆç†çš„åˆå§‹åŒ–ï¼Œä½¿ç”¨`Sequential`ç­‰æ–¹æ³•æ·»åŠ çš„å±‚ä¸éœ€è¦é¢å¤–åˆå§‹åŒ–==ï¼ˆï¼Ÿï¼‰==ã€‚

##### Xavieréšæœºåˆå§‹åŒ–

è¿˜æœ‰ä¸€ç§æ¯”è¾ƒå¸¸ç”¨çš„éšæœºåˆå§‹åŒ–æ–¹æ³•å«ä½œXavieréšæœºåˆå§‹åŒ–[1]ã€‚
å‡è®¾æŸå…¨è¿æ¥å±‚çš„è¾“å…¥ä¸ªæ•°ä¸º$a$ï¼Œè¾“å‡ºä¸ªæ•°ä¸º$b$ï¼ŒXavieréšæœºåˆå§‹åŒ–å°†ä½¿è¯¥å±‚ä¸­æƒé‡å‚æ•°çš„æ¯ä¸ªå…ƒç´ éƒ½éšæœºé‡‡æ ·äºå‡åŒ€åˆ†å¸ƒ

$$U\left(-\sqrt{\frac{6}{a+b}}, \sqrt{\frac{6}{a+b}}\right).$$

å®ƒçš„è®¾è®¡ä¸»è¦è€ƒè™‘åˆ°ï¼Œæ¨¡å‹å‚æ•°åˆå§‹åŒ–åï¼Œæ¯å±‚è¾“å‡ºçš„æ–¹å·®ä¸è¯¥å—è¯¥å±‚è¾“å…¥ä¸ªæ•°å½±å“ï¼Œä¸”æ¯å±‚æ¢¯åº¦çš„æ–¹å·®ä¹Ÿä¸è¯¥å—è¯¥å±‚è¾“å‡ºä¸ªæ•°å½±å“ã€‚



***

### æ¨¡å‹æ“ä½œç›¸å…³

æ­¤å¤„æ ¹æ®åŸä¹¦å’Œpytorchå®˜æ–¹æ–‡æ¡£ï¼Œè®°å½•pytorchä¸­æ¨¡å‹æ“ä½œç›¸å…³çš„ç±»ä¸å‡½æ•°ã€‚

#### æ¨¡å‹æ„é€ 

##### ç»§æ‰¿`Moudle`ç±»æ„é€ æ¨¡å‹

`nn`æ¨¡å—æä¾›`Moudle`ç±»æ¥æ„é€ æ¨¡å‹ï¼Œè¿™æ˜¯ç¥ç»ç½‘ç»œæ¨¡å—çš„åŸºç±»ï¼Œå¯ä»¥é€šè¿‡ç»§æ‰¿å®ƒè‡ªå®šä¹‰æ¨¡å‹ã€‚è‡ªå®šä¹‰çš„ç±»éœ€è¦åŒ…æ‹¬`__init__`å‡½æ•°å’Œ`forward`å‡½æ•°ï¼Œç”¨äºåˆå§‹åŒ–æ¨¡å‹å’Œå®šä¹‰å‰å‘è®¡ç®—ï¼ˆæ­£å‘ä¼ æ’­ï¼‰ã€‚è‡ªå®šä¹‰ç±»ä¸­æ— é¡»å®šä¹‰åå‘ä¼ æ’­å‡½æ•°ï¼Œç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆåå‘ä¼ æ’­æ‰€éœ€çš„`backward`å‡½æ•°ã€‚

`Moudle`ç±»çš„å­ç±»å¯ä»¥ä¸æ˜¯å•ç‹¬ä¸€å±‚ï¼Œä¹Ÿå¯ä»¥æ˜¯ä¸€æ•´ä¸ªæ¨¡å‹æˆ–è€…å¤šå±‚ç»„æˆçš„éƒ¨åˆ†ã€‚

###### è‡ªå®šä¹‰å±‚

ä¸ç»§æ‰¿`Moudle`ç±»æ„é€ æ¨¡å‹ç›¸ä¼¼ã€‚

å¦‚éœ€æ„å»ºå«æœ‰æ¨¡å‹å‚æ•°çš„è‡ªå®šä¹‰å±‚ï¼Œé™¤äº†ç›´æ¥ä½¿ç”¨`self.x(å‚æ•°å)=nn.Parameter(...)`å¤–ï¼Œè¿˜å¯ä»¥ä½¿ç”¨`ParameterList`å’Œ`ParameterDict`åˆ†åˆ«å®šä¹‰å‚æ•°çš„åˆ—è¡¨å’Œå­—å…¸ã€‚

`ParameterList`æ¥æ”¶ä¸€ä¸ª`Parameter`å®ä¾‹çš„åˆ—è¡¨ä½œä¸ºè¾“å…¥ç„¶åå¾—åˆ°ä¸€ä¸ªå‚æ•°åˆ—è¡¨ï¼Œä½¿ç”¨çš„æ—¶å€™å¯ä»¥ç”¨ç´¢å¼•æ¥è®¿é—®æŸä¸ªå‚æ•°ï¼Œå¦å¤–ä¹Ÿå¯ä»¥ä½¿ç”¨`append`å’Œ`extend`åœ¨åˆ—è¡¨åé¢æ–°å¢å‚æ•°ã€‚



##### `Moudle`çš„å­ç±»

Pytorchæä¾›äº†ç»§æ‰¿è‡ª`Moudle`çš„å¯ä»¥æ–¹ä¾¿æ„å»ºæ¨¡å‹çš„ç±»ï¼š å¦‚`Sequential`ã€`ModuleList`å’Œ`ModuleDict`ç­‰ç­‰ã€‚

###### `Sequential` ç±»

æ¨¡å‹çš„å‰å‘è®¡ç®—å¦‚æœåªæ˜¯ç®€å•ä¸²è”å„ä¸ªå±‚ï¼Œä½¿ç”¨`Sequential`ç±»å¯ä»¥ç®€å•åœ°å®šä¹‰æ¨¡å‹ã€‚å®ƒæ¥å—å­æ¨¡å—çš„æœ‰åºå­—å…¸æˆ–è€…ä»¥ä¸€ç³»åˆ—å­æ¨¡å—ä½œä¸ºå‚æ•°æ¥é€ä¸€æ·»åŠ `Moudle`çš„å®ä¾‹ï¼Œç„¶åæŒ‰ç…§å®ä¾‹çš„æ·»åŠ é¡ºåºè¿›è¡Œæ¨¡å‹çš„å‰å‘è®¡ç®—ã€‚

ç¤ºä¾‹ï¼Œä»¥ä¸‹ä¸¤ç§æ–¹å¼ç­‰ä»·ï¼š

```python
net = nn.Sequential(
        nn.Linear(784, 256),
        nn.ReLU(),
        nn.Linear(256, 10),
    	nn.ReLU(),
        )

net = nn.Sequential(OrderedDict([
        ('linear1', nn.Linear(784, 256),
        ('relu1', nn.ReLU()),
        ('linear2', nn.Linear(256, 10)),
        ('relu2', nn.ReLU())
        ]))
```

* æ³¨ï¼šè™½ç„¶`Sequential`ç­‰ç±»å¯ä»¥ä½¿æ¨¡å‹æ„é€ æ›´åŠ ç®€å•ï¼Œä½†ç›´æ¥ç»§æ‰¿`Module`ç±»å¯ä»¥æå¤§åœ°æ‹“å±•æ¨¡å‹æ„é€ çš„çµæ´»æ€§ã€‚

###### `ModuleList`ç±»

`ModuleList`æ¥æ”¶ä¸€ä¸ªå­æ¨¡å—çš„åˆ—è¡¨ä½œä¸ºè¾“å…¥ï¼Œå¯ä»¥ç±»ä¼¼Listé‚£æ ·è¿›è¡Œappendå’Œextendæ“ä½œ:

``` python
net = nn.ModuleList([nn.Linear(784, 256), nn.ReLU()])
net.append(nn.Linear(256, 10)) # # ç±»ä¼¼Listçš„appendæ“ä½œ
print(net[-1])  # ç±»ä¼¼Listçš„ç´¢å¼•è®¿é—®
print(net)
# net(torch.zeros(1, 784)) # ä¼šæŠ¥NotImplementedError
```

è¾“å‡ºï¼š

```
Linear(in_features=256, out_features=10, bias=True)
ModuleList(
  (0): Linear(in_features=784, out_features=256, bias=True)
  (1): ReLU()
  (2): Linear(in_features=256, out_features=10, bias=True)
)
```

ç›¸è¾ƒäº`Sequential`ç±»ï¼Œ`ModuleList`ä»…ä»…æ˜¯ä¸€ä¸ªå‚¨å­˜å„ç§æ¨¡å—çš„åˆ—è¡¨ï¼Œè¿™äº›æ¨¡å—ä¹‹é—´æ²¡æœ‰è”ç³»ä¹Ÿæ²¡æœ‰é¡ºåºï¼ˆæ‰€ä»¥ä¸ç”¨ä¿è¯ç›¸é‚»å±‚çš„è¾“å…¥è¾“å‡ºç»´åº¦åŒ¹é…ï¼‰ï¼Œforward`åŠŸèƒ½éœ€è¦è‡ªå·±å®ç°ã€‚è¯¥ç±»çš„ä½œç”¨åœ¨äºè®©ç½‘ç»œå®šä¹‰å‰å‘ä¼ æ’­æ—¶æ›´åŠ çµæ´»ã€‚

* æ³¨ï¼šåŠ å…¥åˆ°`ModuleList`é‡Œé¢çš„æ‰€æœ‰æ¨¡å—çš„å‚æ•°ä¼šè¢«è‡ªåŠ¨æ·»åŠ åˆ°æ•´ä¸ªç½‘ç»œä¸­ã€‚

###### `ModuleDict`ç±»

`ModuleDict`æ¥æ”¶ä¸€ä¸ªå­æ¨¡å—çš„å­—å…¸ä½œä¸ºè¾“å…¥, ç„¶åä¹Ÿå¯ä»¥ç±»ä¼¼å­—å…¸é‚£æ ·è¿›è¡Œæ·»åŠ è®¿é—®æ“ä½œã€‚åŒæ ·ï¼Œä»…æ˜¯å­˜æ”¾ä¸€äº›æ¨¡å—çš„å­—å…¸ï¼Œ`forward`éœ€è¦è‡ªè¡Œå®šä¹‰ã€‚

* æ³¨ï¼šåŒæ ·ï¼Œ`ModuleDict`é‡Œçš„æ‰€æœ‰æ¨¡å—çš„å‚æ•°ä¼šè¢«è‡ªåŠ¨æ·»åŠ åˆ°æ•´ä¸ªç½‘ç»œä¸­ã€‚

```python
net = nn.ModuleDict({
    'linear': nn.Linear(784, 256),
    'act': nn.ReLU(),
})
net['output'] = nn.Linear(256, 10) # æ·»åŠ 
print(net['linear']) # è®¿é—®
```

#### 

#### æ¨¡å‹å‚æ•°

##### è®¿é—®

* å¯¹äº`Sequential`å®ä¾‹ä¸­å«æ¨¡å‹å‚æ•°çš„å±‚ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡`Module`ç±»çš„`parameters()`æˆ–è€…`named_parameters`æ–¹æ³•æ¥è®¿é—®æ‰€æœ‰å‚æ•°ï¼ˆä»¥è¿­ä»£å™¨çš„å½¢å¼è¿”å›ï¼‰ï¼Œåè€…é™¤äº†è¿”å›å‚æ•°`Tensor`å¤–è¿˜ä¼šè¿”å›å…¶åå­—ã€‚
* å¯¹äºä½¿ç”¨`Sequential`ç±»æ„é€ çš„ç¥ç»ç½‘ç»œï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ–¹æ‹¬å·`[]`æ¥è®¿é—®ç½‘ç»œçš„ä»»ä¸€å±‚ã€‚

##### åˆå§‹åŒ–

Pytorchçš„`init`æ¨¡å—æä¾›äº†å¤šç§é¢„è®¾çš„åˆå§‹åŒ–æ–¹æ³•ã€‚å¸¸ç”¨çš„å³æ­£æ€åˆ†å¸ƒéšæœºæ•°ï¼ˆå¯¹weightï¼‰å’Œæ¸…é›¶ï¼ˆå¯¹biasï¼‰

```python
for name, param in net.named_parameters():
    if 'weight' in name:
        init.normal_(param, mean=0, std=0.01) # meanå‡å€¼ stdæ ‡å‡†å·®
        print(name, param.data)
    if 'bias' in name:
        init.constant_(param, val=0)
        print(name, param.data)
```

==è‡ªå®šä¹‰åˆå§‹åŒ–æ–¹æ³•æ—¶ï¼Œæ³¨æ„æ­¤æ“ä½œä¸è®°å½•æ¢¯åº¦==

##### å…±äº«

*  `Module`ç±»çš„`forward`å‡½æ•°é‡Œå¤šæ¬¡è°ƒç”¨åŒä¸€ä¸ªå±‚ï¼Œå¯ä»¥åšåˆ°å…±äº«æ¨¡å‹å‚æ•°
* ä¼ å…¥`Sequential`çš„æ¨¡å—æ˜¯åŒä¸€ä¸ª`Module`å®ä¾‹ï¼Œåˆ™å‚æ•°å…±äº«



#### è¯»å–å’Œå­˜å‚¨

å°†è®­ç»ƒå¥½çš„æ¨¡å‹å‚æ•°è¿›è¡Œå­˜å‚¨ï¼Œæˆ–è€…è¯»å–å·²ç»è®­ç»ƒå¥½çš„æ¨¡å‹ã€‚

é€šå¸¸å­˜å‚¨æ–‡ä»¶ä¸º`.pt`åç¼€å

##### è¯»å†™`Tensor`

å¯ä»¥ç›´æ¥ä½¿ç”¨`save`å‡½æ•°å’Œ`load`å‡½æ•°åˆ†åˆ«å­˜å‚¨å’Œè¯»å–`Tensor`

ä½¿ç”¨`save`å¯ä»¥ä¿å­˜å„ç§å¯¹è±¡,åŒ…æ‹¬æ¨¡å‹ã€å¼ é‡å’Œå­—å…¸ç­‰ã€‚è€Œ`load`ä½¿ç”¨pickle unpickleå·¥å…·å°†pickleçš„å¯¹è±¡æ–‡ä»¶ååºåˆ—åŒ–ä¸ºå†…å­˜ã€‚

```python
# example
x = torch.ones(3)
y = torch.zeros(4)
torch.save([x,y], 'x.pt')
xylist = torch.load('x.pt')
```

##### è¯»å†™æ¨¡å‹

==åœ¨\9_MLP_Dropout_Pytorch.ipynbè¯•éªŒ==

`Module`çš„å¯å­¦ä¹ å‚æ•°(å³æƒé‡å’Œåå·®)ï¼Œæ¨¡å—æ¨¡å‹åŒ…å«åœ¨å‚æ•°ä¸­(é€šè¿‡`model.parameters()`è®¿é—®)ã€‚`state_dict`æ˜¯ä¸€ä¸ªä»å‚æ•°åç§°éšå°„åˆ°å‚æ•°`Tesnor`çš„å­—å…¸å¯¹è±¡ã€‚

åªæœ‰å…·æœ‰å¯å­¦ä¹ å‚æ•°çš„å±‚(å·ç§¯å±‚ã€çº¿æ€§å±‚ç­‰)æ‰æœ‰`state_dict`ä¸­çš„æ¡ç›®ã€‚ä¼˜åŒ–å™¨(`optim`)ä¹Ÿæœ‰ä¸€ä¸ª`state_dict`ï¼Œå…¶ä¸­åŒ…å«å…³äºä¼˜åŒ–å™¨çŠ¶æ€ä»¥åŠæ‰€ä½¿ç”¨çš„è¶…å‚æ•°ã€‚

ä¿å­˜æ¨¡å‹æœ‰ä¸¤ç§æ–¹å¼ï¼š**ä¿å­˜å‚æ•°**==ï¼ˆå®˜æ–¹æ¨èï¼Œä½†æ•™æä¸­ä»£ç å¯èƒ½å·²ç»è¿‡æ—¶ï¼‰==å’Œ**ä¿å­˜æ•´ä¸ªæ¨¡å‹**

```python
# 1.ä¿å­˜å’ŒåŠ è½½state_dictï¼ˆå‚æ•°çŠ¶æ€ï¼‰

#ä¿å­˜ï¼Œä½¿ç”¨torch.save
torch.save(model.state_dict(), PATH) # ä¸€èˆ¬è€Œè¨€ï¼Œæ–‡ä»¶åç¼€åæ˜¯ptæˆ–pth

#åŠ è½½ï¼Œä½¿ç”¨load)state_dict
model = TheModelClass(*args, **kwargs)
model.load_state_dict(torch.load(PATH))
```

==pytorch1.7åŠä¹‹åç‰ˆæœ¬`TheModelClass(*args, **kwargs)`ä¸å¯ç”¨ï¼ˆä»Docä¸­åˆ é™¤ï¼Œä¹Ÿå¯èƒ½è¿™åªæ˜¯ä»£æŒ‡å®šä¹‰æ¨¡å‹ï¼Œå¹¶éå†…ç½®APIï¼Ÿï¼‰è¿˜æ˜¯å…ˆæŒ‰å±‚å®šä¹‰å¥½netæ‰å¯¼å…¥å‚æ•°å§==


```python
# 2.ä¿å­˜å’ŒåŠ è½½æ•´ä¸ªæ¨¡å‹ï¼Œå³ä½¿ç”¨save,load,åŒè¯»å†™tensor
torch.save(model, PATH)
model = torch.load(PATH)
```

æ³¨ï¼šPATHæ ¼å¼ä¸ç”¨å‰ç½®"/"æˆ–"\\"ï¼ˆå¦‚å­˜æ”¾åœ¨æ–°çš„å­ç›®å½•ï¼Œéœ€è¦å…ˆæ–°å»ºå¥½å¦åˆ™ä¼šæŠ¥é”™ï¼‰ï¼Œå¦‚

```python
torch.save(net.state_dict(),'Model_Save/9_net_param.pt')
```



#### GPUè®¡ç®—

* PyTorchå¯ä»¥æŒ‡å®šç”¨æ¥å­˜å‚¨å’Œè®¡ç®—çš„è®¾å¤‡ï¼Œå¦‚ä½¿ç”¨å†…å­˜çš„CPUæˆ–è€…ä½¿ç”¨æ˜¾å­˜çš„GPUã€‚PyTorché»˜è®¤ä¼šå°†æ•°æ®åˆ›å»ºåœ¨å†…å­˜ï¼Œç„¶ååˆ©ç”¨CPUæ¥è®¡ç®—ã€‚
* PyTorchè¦æ±‚è®¡ç®—çš„æ‰€æœ‰è¾“å…¥æ•°æ®éƒ½åœ¨å†…å­˜æˆ–åŒä¸€å—æ˜¾å¡çš„æ˜¾å­˜ä¸Šã€‚

##### æŸ¥çœ‹æœ¬æœºæ˜¾å¡ï¼ˆNç³»ï¼‰çŠ¶æ€å‘½ä»¤

```shell
nvidia-smi 
```

##### æŸ¥çœ‹æ˜¯å¦æ”¯æŒGPUè®¡ç®—

```python
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# ä¹‹ååœ¨è®¡ç®—æ—¶å¯ä»¥ä½¿ç”¨to(device)
```

##### `Tensor`çš„GPUè®¡ç®—

é»˜è®¤æƒ…å†µä¸‹ï¼Œ`Tensor`ä¼šè¢«å­˜åœ¨å†…å­˜ä¸Šã€‚

* ä½¿ç”¨`.cuda()`å¯ä»¥å°†CPUä¸Šçš„`Tensor`è½¬æ¢ï¼ˆå¤åˆ¶ï¼‰åˆ°GPUä¸Šã€‚å¦‚æœæœ‰å¤šå—GPUï¼Œæˆ‘ä»¬ç”¨`.cuda(i)`æ¥è¡¨ç¤ºç¬¬ $i$ å—GPUåŠç›¸åº”çš„æ˜¾å­˜ï¼ˆ$i$ä»0å¼€å§‹ï¼‰ä¸”`cuda(0)`å’Œ`cuda()`ç­‰ä»·ã€‚

  ```python
  x = x.cuda(0)
  ```

* æˆ‘ä»¬å¯ä»¥ç›´æ¥åœ¨åˆ›å»ºçš„æ—¶å€™å°±æŒ‡å®šè®¾å¤‡ã€‚

  ```python
  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
  
  x = torch.tensor([1, 2, 3], device=device)
  # or
  x = torch.tensor([1, 2, 3]).to(device)
  ```

* ä¹Ÿå¯ä»¥ä½¿ç”¨`.to(device)`ï¼Œå°†ç½‘ç»œè½¬è‡³GPUï¼ˆ`.to(device)`ä¸`.cuda()`åŒºåˆ«åœ¨äºå‰è€…å¯ä»¥è½¬å›CPUï¼‰

* å¦‚æœå¯¹åœ¨GPUä¸Šçš„æ•°æ®è¿›è¡Œè¿ç®—ï¼Œé‚£ä¹ˆç»“æœè¿˜æ˜¯å­˜æ”¾åœ¨GPUä¸Šã€‚

**éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå­˜å‚¨åœ¨ä¸åŒä½ç½®ä¸­çš„æ•°æ®æ˜¯ä¸å¯ä»¥ç›´æ¥è¿›è¡Œè®¡ç®—çš„ã€‚**å³å­˜æ”¾åœ¨CPUä¸Šçš„æ•°æ®ä¸å¯ä»¥ç›´æ¥ä¸å­˜æ”¾åœ¨GPUä¸Šçš„æ•°æ®è¿›è¡Œè¿ç®—ï¼Œä½äºä¸åŒGPUä¸Šçš„æ•°æ®ä¹Ÿæ˜¯ä¸èƒ½ç›´æ¥è¿›è¡Œè®¡ç®—çš„ã€‚

##### æ¨¡å‹çš„GPUè®¡ç®—

åŒ`Tensor`ç±»ä¼¼ï¼ŒPyTorchçš„æ•´ä¸ªæ¨¡å‹ç½‘ç»œä¹Ÿå¯ä»¥é€šè¿‡`.cuda`è½¬æ¢åˆ°GPUä¸Šã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡æ£€æŸ¥æ¨¡å‹çš„å‚æ•°çš„`device`å±æ€§æ¥æŸ¥çœ‹å­˜æ”¾æ¨¡å‹çš„è®¾å¤‡ã€‚**éœ€è¦ä¿è¯æ¨¡å‹è¾“å…¥çš„`Tensor`å’Œæ¨¡å‹éƒ½åœ¨åŒä¸€è®¾å¤‡ä¸Šï¼Œå¦åˆ™ä¼šæŠ¥é”™ã€‚ï¼ˆæŠŠèƒ½è§åˆ°çš„éƒ½`.cuda`/`.to(gpu)`äº†ï¼Œç”šè‡³åŒ…æ‹¬losså‡½æ•°ï¼‰**

==å¤šæ˜¾å¡è‡ªåŠ¨åˆ†é…é—®é¢˜ï¼Ÿ==

åœ¨å°†æ¨¡å‹å®ä¾‹æ”¾åˆ°GPUä¸Šæ—¶ï¼Œå³ç”¨`model.cuda()`æ—¶ï¼Œåªä¼šå°†__init__ä¸­çš„æœ‰selfå‰ç¼€çš„å±æ€§åŠå‡½æ•°æ”¾åˆ°GPUä¸Šï¼Œå¯¹äºå…¶ä»–çš„å‡½æ•°æ— ä½œç”¨ã€‚æ‰€ä»¥é`__init__`ä¸­å®šä¹‰çš„å‡½æ•°å’Œæ•°æ®éœ€è¦å•ç‹¬çš„æ”¾å…¥GPUè¯­å¥ã€‚

***

## CNNå·ç§¯ç¥ç»ç½‘ç»œ

å·ç§¯ç¥ç»ç½‘ç»œï¼ˆconvolutional neural networkï¼‰ï¼šå«æœ‰å·ç§¯å±‚çš„ç½‘ç»œ

### å·ç§¯å±‚åŸºç¡€æ¦‚å¿µ

#### äº’ç›¸å…³è¿ç®—

é€šå¸¸åœ¨å·ç§¯å±‚ä¸­ä½¿ç”¨çš„ä¸æ˜¯å·ç§¯è¿ç®—ï¼ˆconvolution)ï¼Œè€Œæ˜¯æ›´ä¸ºç›´è§‚çš„**äº’ç›¸å…³è¿ç®—**ï¼ˆcross-correlationï¼‰ã€‚å³å·ç§¯æ ¸ï¼ˆconvolution kernelï¼‰å’Œè¦†ç›–éƒ¨åˆ†åšç‚¹ç§¯â€”â€”

å·ç§¯çª—å£ä»è¾“å…¥æ•°ç»„çš„æœ€å·¦ä¸Šæ–¹å¼€å§‹ï¼ŒæŒ‰ä»å·¦å¾€å³ã€ä»ä¸Šå¾€ä¸‹çš„é¡ºåºï¼Œä¾æ¬¡åœ¨è¾“å…¥æ•°ç»„ä¸Šæ»‘åŠ¨ã€‚å½“å·ç§¯çª—å£æ»‘åŠ¨åˆ°æŸä¸€ä½ç½®æ—¶ï¼Œçª—å£ä¸­çš„è¾“å…¥å­æ•°ç»„ä¸æ ¸æ•°ç»„æŒ‰å…ƒç´ ç›¸ä¹˜å¹¶æ±‚å’Œï¼Œå¾—åˆ°è¾“å‡ºæ•°ç»„ä¸­ç›¸åº”ä½ç½®çš„å…ƒç´ ã€‚

<img src="figs/image-20220706104425982.png" alt="image-20220706104425982" style="zoom: 80%;" />

äºŒç»´å·ç§¯å±‚å°†è¾“å…¥å’Œå·ç§¯æ ¸åšäº’ç›¸å…³è¿ç®—ï¼Œå¹¶åŠ ä¸Šä¸€ä¸ªæ ‡é‡åå·®æ¥å¾—åˆ°è¾“å‡ºã€‚å·ç§¯å±‚çš„æ¨¡å‹å‚æ•°åŒ…æ‹¬äº†å·ç§¯æ ¸å’Œæ ‡é‡åå·®ã€‚åœ¨è®­ç»ƒæ¨¡å‹çš„æ—¶å€™ï¼Œé€šå¸¸æˆ‘ä»¬å…ˆå¯¹å·ç§¯æ ¸éšæœºåˆå§‹åŒ–ï¼Œç„¶åä¸æ–­è¿­ä»£å·ç§¯æ ¸å’Œåå·®ã€‚

* å¯ä»¥åŸºäº`corr2d`å‡½æ•°å®ç°ä¸€ä¸ªè‡ªå®šä¹‰çš„äºŒç»´å·ç§¯å±‚ã€‚åœ¨æ„é€ å‡½æ•°`__init__`é‡Œå£°æ˜`weight`å’Œ`bias`è¿™ä¸¤ä¸ªæ¨¡å‹å‚æ•°ã€‚å‰å‘è®¡ç®—å‡½æ•°`forward`åˆ™æ˜¯ç›´æ¥è°ƒç”¨`corr2d`å‡½æ•°å†åŠ ä¸Šåå·®ã€‚

å·ç§¯æ ¸çš„å°ºå¯¸é€šå¸¸ä¸ºå¥‡æ•°ï¼š1.ä¸ºäº†same padding æ—¶,å›¾åƒä¸¤è¾¹å¯¹ç§°ï¼›2.ä¿è¯é”šç‚¹åœ¨ä¸­é—´ï¼Œå¥‡æ•°è¿‡æ»¤å™¨çš„é”šç‚¹æ­£å¥½åœ¨ä¸­å¿ƒä½ç½®ï¼Œé¿å…äº†ä½ç½®ä¿¡æ¯å‘ç”Ÿåç§»

#### ç‰¹å¾å›¾ä¸æ„Ÿå—é‡

äºŒç»´å·ç§¯å±‚è¾“å‡ºçš„äºŒç»´æ•°ç»„å¯ä»¥çœ‹ä½œæ˜¯è¾“å…¥åœ¨ç©ºé—´ç»´åº¦ï¼ˆå®½å’Œé«˜ï¼‰ä¸ŠæŸä¸€çº§çš„è¡¨å¾ï¼Œä¹Ÿå«**ç‰¹å¾å›¾**ï¼ˆfeature mapï¼‰ã€‚å½±å“å…ƒç´ $x$çš„å‰å‘è®¡ç®—çš„æ‰€æœ‰å¯èƒ½è¾“å…¥åŒºåŸŸï¼ˆå¯èƒ½å¤§äºè¾“å…¥çš„å®é™…å°ºå¯¸ï¼‰å«åš$x$çš„**æ„Ÿå—é‡**ï¼ˆreceptive fieldï¼‰ã€‚ä»¥ä¸Šå›¾ä¸ºä¾‹ï¼ŒImageä¸­çš„é»„è‰²$3 \times 3$çŸ©é˜µæ˜¯è¾“å‡ºä¸­ç²‰è‰²å…ƒç´ çš„æ„Ÿå—é‡ã€‚ç”±æ­¤å¯è§ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å åŠ æ›´æ·±çš„å·ç§¯ç¥ç»ç½‘ç»œï¼Œä½¿ç‰¹å¾å›¾ä¸­å•ä¸ªå…ƒç´ çš„æ„Ÿå—é‡å˜å¾—æ›´åŠ å¹¿é˜”ï¼Œä»è€Œæ•æ‰è¾“å…¥ä¸Šæ›´å¤§å°ºå¯¸çš„ç‰¹å¾ã€‚

ï¼ˆå¸¸ä½¿ç”¨â€œå…ƒç´ â€ä¸€è¯æ¥æè¿°æ•°ç»„æˆ–çŸ©é˜µä¸­çš„æˆå‘˜ã€‚åœ¨ç¥ç»ç½‘ç»œçš„æœ¯è¯­ä¸­ï¼Œè¿™äº›å…ƒç´ ä¹Ÿå¯ç§°ä¸ºâ€œå•å…ƒâ€ã€‚ï¼‰

#### å¡«å……

å¡«å……ï¼ˆpaddingï¼‰ï¼ŒæŒ‡åœ¨è¾“å…¥çš„é«˜å’Œå®½ä¸¤ä¾§å¡«å……å…ƒç´ ï¼ˆé€šå¸¸ä¸º0å…ƒç´ ï¼‰ã€‚éšç€å·ç§¯æ¬¡æ•°å¢å¤šï¼Œç‰¹å¾å›¾è¾“å‡ºå°ºå¯¸ä¼šå˜å°ï¼Œé€šè¿‡paddingå¯ä»¥æ§åˆ¶è¾“å‡ºç‰¹å¾å›¾çš„å¤§å°ï¼ˆå¦‚æ§åˆ¶Stride=1çš„å·ç§¯ç‰¹å¾å›¾è¾“å‡ºä¸è¾“å…¥å°ºå¯¸ç›¸ç­‰ï¼‰ã€‚

è¿˜æœ‰ä¸€ç§éœ€è¦paddingçš„æƒ…å†µï¼Œå¦‚ç»™å®š$5 \times 5$çš„è¾“å…¥å›¾ï¼Œ$kernel=2 \times 2$ï¼Œ$stride=2$æ—¶ï¼Œå¦‚æœä¸é€šè¿‡paddingè¡¥0ï¼Œæœ€è¾¹ç¼˜ä¸€è¡Œä¸€åˆ—ç”±äº5ä¸æ•´é™¤2ï¼Œæ— æ³•è¢«å·ç§¯ï¼Œä¼šé€ æˆæ•°æ®ä¸¢å¤±ã€‚

>**Tensorflowä¸­çš„paddingæœ‰å¤šç§æ¨¡å¼**
>
>* full modeï¼Œä»filterå’Œimageåˆšç›¸äº¤å¼€å§‹åšå·ç§¯
>* same modeï¼Œå½“filterçš„ä¸­å¿ƒKä¸imageçš„è¾¹è§’é‡åˆæ—¶ï¼Œå¼€å§‹åšå·ç§¯è¿ç®—ã€‚æ­¥é•¿ä¸º1æ—¶å·ç§¯ä¹‹åè¾“å‡ºçš„feature mapå°ºå¯¸ä¿æŒä¸å˜(ç›¸å¯¹äºè¾“å…¥å›¾ç‰‡)
>* validï¼Œå½“filterå…¨éƒ¨åœ¨imageé‡Œé¢çš„æ—¶å€™ï¼Œè¿›è¡Œå·ç§¯è¿ç®—
>
>**Pytorchä¸­çš„paddingå‚æ•°ä¸ºæ•°å­—0ã€1ã€2ï¼Œæ„ä¸ºåŸå›¾å‘¨å›´éœ€è¦å¡«å……çš„æ ¼å­è¡Œï¼ˆåˆ—ï¼‰æ•°**

#### æ­¥å¹…

æ­¥å¹…ï¼ˆstrideï¼‰ï¼šå·ç§¯å±‚è¶…å‚æ•°ï¼Œå³æ»‘åŠ¨çš„æ­¥é•¿ï¼Œæ§åˆ¶å·ç§¯æ ¸åœ¨è¾“å…¥å›¾åƒä¸Šä»¥æ€æ ·çš„æ­¥å¹…ç§»åŠ¨ã€‚

* Strideè¶Šå°ï¼Œå·ç§¯é‡å éƒ¨åˆ†è¾ƒå¤šï¼Œæå–çš„ç‰¹å¾å¤šï¼Œæ—¶é—´æ•ˆç‡å¯èƒ½é™ä½ã€‚
* Strideè¶Šå¤§ï¼Œé‡å åŒºåŸŸè¶Šå°‘ï¼Œå‡å°‘è®¡ç®—é‡ä½†å¯èƒ½æ¼æ‰ä¿¡æ¯ã€‚

#### å¤šé€šé“è¾“å…¥è¾“å‡º

å·ç§¯å¸¸ç”¨äºè®¡ç®—æœºè§†è§‰ï¼Œæ‰€ä»¥è¾“å…¥æ•°æ®çš„ç»´åº¦é€šå¸¸ä¼šæ›´é«˜ï¼Œå¦‚å½©è‰²å›¾åƒé™¤é«˜å’Œå®½çš„äºŒç»´å¤–è¿˜æœ‰RGBä¸‰ä¸ªé¢œè‰²é€šé“ã€‚

* **å¤šé€šé“è¾“å…¥**æ—¶ï¼Œéœ€è¦æ„é€ ä¸€ä¸ªè¾“å…¥é€šé“æ•°ä¸è¾“å…¥æ•°æ®çš„é€šé“æ•°ç›¸åŒçš„å·ç§¯æ ¸ï¼Œä»è€Œèƒ½å¤Ÿä¸å«å¤šé€šé“çš„è¾“å…¥æ•°æ®åšäº’ç›¸å…³è¿ç®—ã€‚åœ¨å„ä¸ªé€šé“ä¸Šå¯¹è¾“å…¥çš„äºŒç»´æ•°ç»„å’Œå·ç§¯æ ¸çš„äºŒç»´æ ¸æ•°ç»„åšäº’ç›¸å…³è¿ç®—ï¼Œå†å°†è¿™$c_i$ä¸ªäº’ç›¸å…³è¿ç®—çš„äºŒç»´è¾“å‡ºæŒ‰é€šé“ç›¸åŠ ï¼Œå¾—åˆ°ä¸€ä¸ªäºŒç»´æ•°ç»„ï¼Œä½œä¸ºè¾“å‡ºã€‚

![5.3_conv_multi_in](figs/5.3_conv_multi_in-16570769879311.svg)

* **å¤šé€šé“è¾“å‡º**æ—¶ï¼Œè®¾å·ç§¯æ ¸è¾“å…¥é€šé“æ•°å’Œè¾“å‡ºé€šé“æ•°åˆ†åˆ«ä¸º$c_i$å’Œ$c_o$ï¼Œé«˜å’Œå®½åˆ†åˆ«ä¸º$k_h$å’Œ$k_w$ã€‚ä¸ºæ¯ä¸ªè¾“å‡ºé€šé“åˆ†åˆ«åˆ›å»ºå½¢çŠ¶ä¸º$c_i\times k_h\times k_w$çš„æ ¸æ•°ç»„ã€‚å°†å®ƒä»¬åœ¨è¾“å‡ºé€šé“ç»´ä¸Šè¿ç»“ï¼Œå·ç§¯æ ¸çš„å½¢çŠ¶å³$c_o\times c_i\times k_h\times k_w$ã€‚åœ¨åšäº’ç›¸å…³è¿ç®—æ—¶ï¼Œæ¯ä¸ªè¾“å‡ºé€šé“ä¸Šçš„ç»“æœç”±å·ç§¯æ ¸åœ¨è¯¥è¾“å‡ºé€šé“ä¸Šçš„æ ¸æ•°ç»„ä¸æ•´ä¸ªè¾“å…¥æ•°ç»„è®¡ç®—è€Œæ¥ã€‚

* **å¤šå±‚å·ç§¯è¿ç®—**

  æŸä¸€å±‚ä½¿ç”¨nä¸ªä¸åŒkernelè¿›è¡Œå·ç§¯ï¼Œå¯ä»¥å¾—åˆ°nå¼ ä¸åŒçš„feature mapï¼Œè¿™nå¼ å åœ¨ä¸€èµ·ä½œä¸ºä¸‹ä¸€å±‚è¾“å…¥ç‰¹å¾å›¾ã€‚ä¸‹ä¸€å±‚å·ç§¯çš„kernelæ·±åº¦å’Œä¸Šä¸€å±‚è¾“å‡ºfeature mapæ·±åº¦ç›¸ç­‰ã€‚

  éšç€å·ç§¯å±‚æ·±å…¥ï¼Œé«˜å’Œå®½å˜å°ï¼Œå¸¸å¸¸å¢åŠ è¾“å‡ºé€šé“ä½¿ä¸¤ä¸ªå·ç§¯å±‚å‚æ•°å°ºå¯¸ç±»ä¼¼ã€‚ï¼ˆLeNetæ¨¡å‹ï¼‰

  <img src="figs/image-20220706112431923.png" alt="image-20220706112431923" style="zoom: 80%;" />

* **$1\times 1$å·ç§¯**

  $1\times 1$å·ç§¯å¤±å»äº†å·ç§¯å±‚å¯ä»¥è¯†åˆ«é«˜å’Œå®½ç»´åº¦ä¸Šç›¸é‚»å…ƒç´ æ„æˆçš„æ¨¡å¼çš„åŠŸèƒ½ä¸»ï¼Œå…¶è®¡ç®—ä¸»è¦å‘ç”Ÿåœ¨é€šé“ç»´ä¸Šã€‚

  å‡è®¾æˆ‘ä»¬å°†é€šé“ç»´å½“ä½œç‰¹å¾ç»´ï¼Œå°†é«˜å’Œå®½ç»´åº¦ä¸Šçš„å…ƒç´ å½“æˆæ•°æ®æ ·æœ¬ï¼Œ**é‚£ä¹ˆ$1\times 1$å·ç§¯å±‚çš„ä½œç”¨ä¸å…¨è¿æ¥å±‚ç­‰ä»·**ã€‚

#### Pytorchå®ç°

`torch.nn`æä¾›äº†1dã€2dã€3då·ç§¯å±‚ï¼Œä»¥2dä¸ºä¾‹ï¼š

```python
torch.nn.Conv2d(    in_channels,  #è¾“å…¥æ•°æ®çš„é€šé“æ•°ï¼ˆå¦‚å½©è‰²å›¾ç‰‡ï¼Œä¸€èˆ¬ä¸º3ï¼‰
					out_channels, #è¾“å‡ºæ•°æ®çš„é€šé“æ•°ï¼ˆå°±æ˜¯æˆ‘æƒ³è®©è¾“å‡ºå¤šå°‘é€šé“ï¼Œå°±è®¾ç½®ä¸ºå¤šå°‘ï¼‰
					kernel_size,  #å·ç§¯æ ¸çš„å°ºå¯¸ï¼ˆå¦‚ï¼ˆ3ï¼Œ2ï¼‰ï¼Œ3ä¸ï¼ˆ3,3ï¼‰ç­‰åŒï¼‰
					stride = 1,   #å·ç§¯æ­¥é•¿ï¼Œå°±æ˜¯å·ç§¯æ“ä½œæ—¶æ¯æ¬¡ç§»åŠ¨çš„æ ¼å­æ•°
					padding = 0,  #åŸå›¾å‘¨å›´éœ€è¦å¡«å……çš„æ ¼å­è¡Œï¼ˆåˆ—ï¼‰æ•°ï¼Œæ— å¡«å……çš„è¯å·ç§¯åˆ°è¾¹ç¼˜ä¼šç›´æ¥å¿½ç•¥è¯¥è¾¹ç¼˜
					dilation = 1, #ç©ºæ´å·ç§¯çš„ç©ºæ´æŒ‡æ•°ï¼Œä¸€èˆ¬é»˜è®¤ä¸º1å³å¯
					groups = 1,   #åˆ†ç»„å·ç§¯çš„ç»„æ•°ï¼Œä¸€èˆ¬é»˜è®¤è®¾ç½®ä¸º1ï¼Œä¸ç”¨ç®¡
					bias = True,  #å·ç§¯åç½®ï¼Œä¸€èˆ¬è®¾ç½®ä¸ºFalseï¼ŒTrueçš„è¯å¯ä»¥å¢åŠ æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›
					padding_mode = 'zeros'  #è®¾ç½®è¾¹ç¼˜å¡«å……å€¼ä¸º0ï¼Œæˆ–åˆ«çš„æ•°ï¼Œä¸€èˆ¬éƒ½é»˜è®¤è®¾ç½®ä¸º0
				)
```



### æ± åŒ–å±‚

æ± åŒ–å±‚ï¼ˆpoolingï¼‰å°†Feature Mapåˆ’åˆ†ä¸ºå‡ ä¸ªåŒºåŸŸï¼Œ**å–å…¶æœ€å¤§å€¼æˆ–å¹³å‡å€¼**ã€‚åŒå·ç§¯å±‚ä¸€æ ·ï¼Œæ± åŒ–å±‚ä¹Ÿæœ‰paddingå’Œstrideã€‚

å®ƒçš„æå‡ºæ˜¯**ä¸ºäº†ç¼“è§£å·ç§¯å±‚å¯¹ä½ç½®çš„è¿‡åº¦æ•æ„Ÿæ€§ï¼Œå‡å°‘è®¡ç®—é‡ï¼ˆå·ç§¯è·å¾—ç‰¹å¾åç›´æ¥åˆ†ç±»è®¡ç®—é‡ä»ç„¶è¿‡å¤§**ã€‚

å‡å°‘å‚æ•°ï¼Œä¸ä»…å¯ä»¥é™ä½ç»´åº¦ï¼Œè¿˜ä¼šæ”¹å–„è¿‡æ‹Ÿåˆçš„ç»“æœã€‚

å¤„ç†å¤šé€šé“è¾“å…¥æ•°æ®æ—¶ï¼Œæ± åŒ–å±‚å¯¹æ¯ä¸ªè¾“å…¥é€šé“åˆ†åˆ«æ± åŒ–ï¼Œè€Œä¸æ˜¯åƒå·ç§¯å±‚é‚£æ ·å°†å„é€šé“çš„è¾“å…¥æŒ‰é€šé“ç›¸åŠ ã€‚æ‰€ä»¥ï¼Œ**æ± åŒ–å±‚å¾€å¾€åªæ”¹å˜sizeï¼Œä¸æ”¹å˜depth/channelã€‚**

#### Pytorchå®ç°

##### æœ€å¤§æ± åŒ–

`torch.nn`æä¾›äº†1dã€2dã€3dæœ€å¤§æ± åŒ–å±‚ï¼Œä»¥2dä¸ºä¾‹ï¼š

```python
class torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)
```

å‚æ•°ï¼š

- kernel_size(`int` or `tuple`) - max poolingçš„çª—å£å¤§å°
- stride(`int` or `tuple`, `optional`) - max poolingçš„çª—å£ç§»åŠ¨çš„æ­¥é•¿ã€‚é»˜è®¤å€¼æ˜¯`kernel_size`
- padding(`int` or `tuple`, `optional`) - è¾“å…¥çš„æ¯ä¸€æ¡è¾¹è¡¥å……0çš„å±‚æ•°
- dilation(`int` or `tuple`, `optional`) â€“ ä¸€ä¸ªæ§åˆ¶çª—å£ä¸­å…ƒç´ æ­¥å¹…çš„å‚æ•°
- return_indices - å¦‚æœç­‰äº`True`ï¼Œä¼šè¿”å›è¾“å‡ºæœ€å¤§å€¼çš„åºå·ï¼Œå¯¹äºä¸Šé‡‡æ ·æ“ä½œä¼šæœ‰å¸®åŠ©
- ceil_mode - å¦‚æœç­‰äº`True`ï¼Œè®¡ç®—è¾“å‡ºä¿¡å·å¤§å°çš„æ—¶å€™ï¼Œä¼šä½¿ç”¨å‘ä¸Šå–æ•´ï¼Œä»£æ›¿é»˜è®¤çš„å‘ä¸‹å–æ•´çš„æ“ä½œ

##### å¹³å‡æ± åŒ–

`torch.nn`æä¾›äº†1dã€2dã€3då¹³å‡æ± åŒ–å±‚ï¼Œä»¥2dä¸ºä¾‹ï¼š

```python
class torch.nn.AvgPool2d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True)
```

å‚æ•°ï¼š

- kernel_size(`int` or `tuple`) - æ± åŒ–çª—å£å¤§å°
- stride(`int` or `tuple`, `optional`) - avg poolingçš„çª—å£ç§»åŠ¨çš„æ­¥é•¿ã€‚é»˜è®¤å€¼æ˜¯`kernel_size`
- padding(`int` or `tuple`, `optional`) - è¾“å…¥çš„æ¯ä¸€æ¡è¾¹è¡¥å……0çš„å±‚æ•°
- dilation(`int` or `tuple`, `optional`) â€“ ä¸€ä¸ªæ§åˆ¶çª—å£ä¸­å…ƒç´ æ­¥å¹…çš„å‚æ•°
- ceil_mode - å¦‚æœç­‰äº`True`ï¼Œè®¡ç®—è¾“å‡ºä¿¡å·å¤§å°çš„æ—¶å€™ï¼Œä¼šä½¿ç”¨å‘ä¸Šå–æ•´ï¼Œä»£æ›¿é»˜è®¤çš„å‘ä¸‹å–æ•´çš„æ“ä½œ
- count_include_pad - å¦‚æœç­‰äº`True`ï¼Œè®¡ç®—å¹³å‡æ± åŒ–æ—¶ï¼Œå°†åŒ…æ‹¬`padding`å¡«å……çš„0





***

## é™„1-å…¶ä»–å‡½æ•°ç”¨æ³•è®°å½•

### np.random.normal()å‡½æ•°

è¿™æ˜¯çš„npæ˜¯numpyåŒ…çš„ç¼©å†™ï¼Œnp.random.normal()çš„æ„æ€æ˜¯ä¸€ä¸ªæ­£æ€åˆ†å¸ƒï¼Œnormalè¿™é‡Œæ˜¯æ­£æ€çš„æ„æ€ã€‚

å‚æ•°ï¼š`numpy.random.normal(loc,scale,size)`

æ„ä¹‰å¦‚ä¸‹ï¼š 

1. å‚æ•°loc(float)ï¼šæ­£æ€åˆ†å¸ƒçš„å‡å€¼ï¼Œå¯¹åº”ç€è¿™ä¸ªåˆ†å¸ƒçš„ä¸­å¿ƒã€‚loc=0è¯´æ˜è¿™ä¸€ä¸ªä»¥Yè½´ä¸ºå¯¹ç§°è½´çš„æ­£æ€åˆ†å¸ƒï¼Œ
2. å‚æ•°scale(float)ï¼šæ­£æ€åˆ†å¸ƒçš„æ ‡å‡†å·®ï¼Œå¯¹åº”åˆ†å¸ƒçš„å®½åº¦ï¼Œscaleè¶Šå¤§ï¼Œæ­£æ€åˆ†å¸ƒçš„æ›²çº¿è¶ŠçŸ®èƒ–ï¼Œscaleè¶Šå°ï¼Œæ›²çº¿è¶Šé«˜ç˜¦ã€‚
3. å‚æ•°size(int æˆ–è€…æ•´æ•°å…ƒç»„)ï¼šè¾“å‡ºçš„å€¼èµ‹åœ¨shapeé‡Œï¼Œé»˜è®¤ä¸ºNoneã€‚

### meanå‡½æ•°

å¯ä»¥å¯¹çŸ©é˜µï¼ˆå¼ é‡ï¼Ÿï¼‰æ±‚å–å¹³å‡å€¼

### torchçš„ä¸€äº›åŸºæœ¬å‡½æ•°

#### åˆ›å»ºtensor

åˆ›å»ºtensorå‡å¯ä»¥åœ¨åˆ›å»ºçš„æ—¶å€™æŒ‡å®šæ•°æ®ç±»å‹dtypeå’Œå­˜æ”¾device(cpu/gpu)ï¼Œè·Ÿåœ¨sizeså‚æ•°çš„åé¢ï¼Œæœ€å¸¸ç”¨çš„å‡ ç§åˆ›å»ºtensorçš„å‡½æ•°ã€‚

##### torch.empty

åˆ›å»ºæœªåˆå§‹åŒ–çš„tensorï¼Œç”¨æ³•å¦‚x=torch.empty(5,3)

##### torch.rand

åˆ›å»ºéšæœºtensor

##### torch.zerosä¸torch.ones

åˆ›å»ºå…¨éƒ¨ä¸º0æˆ–å…¨éƒ¨ä¸º1çš„tensor

####  ç´¢å¼•

æˆ‘ä»¬è¿˜å¯ä»¥ä½¿ç”¨ç±»ä¼¼NumPyçš„ç´¢å¼•æ“ä½œæ¥è®¿é—®`Tensor`çš„ä¸€éƒ¨åˆ†ï¼Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼š**ç´¢å¼•å‡ºæ¥çš„ç»“æœä¸åŸæ•°æ®å…±äº«å†…å­˜ï¼Œä¹Ÿå³ä¿®æ”¹ä¸€ä¸ªï¼Œå¦ä¸€ä¸ªä¼šè·Ÿç€ä¿®æ”¹ã€‚** 

``` python
y = x[0, :]
y += 1
print(y)
print(x[0, :]) # æºtensorä¹Ÿè¢«æ”¹äº†
```

#### çº¿æ€§ä»£æ•°

å¦å¤–ï¼ŒPyTorchè¿˜æ”¯æŒä¸€äº›çº¿æ€§å‡½æ•°ï¼Œè¿™é‡Œæä¸€ä¸‹ï¼Œå…å¾—ç”¨èµ·æ¥çš„æ—¶å€™è‡ªå·±é€ è½®å­ï¼Œå…·ä½“ç”¨æ³•å‚è€ƒå®˜æ–¹æ–‡æ¡£ã€‚å¦‚ä¸‹è¡¨æ‰€ç¤ºï¼š

|               å‡½æ•°                |               åŠŸèƒ½                |
| :-------------------------------: | :-------------------------------: |
|               trace               |     å¯¹è§’çº¿å…ƒç´ ä¹‹å’Œ(çŸ©é˜µçš„è¿¹)      |
|               diag                |            å¯¹è§’çº¿å…ƒç´              |
|             triu/tril             | çŸ©é˜µçš„ä¸Šä¸‰è§’/ä¸‹ä¸‰è§’ï¼Œå¯æŒ‡å®šåç§»é‡ |
|              mm/bmm               |     çŸ©é˜µä¹˜æ³•ï¼Œbatchçš„çŸ©é˜µä¹˜æ³•     |
| addmm/addbmm/addmv/addr/baddbmm.. |             çŸ©é˜µè¿ç®—              |
|                 t                 |               è½¬ç½®                |
|             dot/cross             |             å†…ç§¯/å¤–ç§¯             |
|              inverse              |             æ±‚é€†çŸ©é˜µ              |
|                svd                |            å¥‡å¼‚å€¼åˆ†è§£             |

#### æ”¹å˜å½¢çŠ¶

ä½¿ç”¨`view()`æ”¹å˜tensorå½¢çŠ¶ï¼Œä½†ä»ç„¶å…±äº«åŒä¸€ä»½data

å³viewåªæ˜¯æ”¹å˜äº†å¯¹å¼ é‡çš„è§‚å¯Ÿè§’åº¦ï¼Œæ²¡æœ‰æ”¹å˜å†…éƒ¨æ•°æ®

æ‰€ä»¥å¦‚æœæˆ‘ä»¬æƒ³è¿”å›ä¸€ä¸ªçœŸæ­£æ–°çš„å‰¯æœ¬ï¼ˆå³ä¸å…±äº«dataå†…å­˜ï¼‰ï¼Œæœ‰ä¸€ä¸ª`reshape()`å¯ä»¥æ”¹å˜å½¢çŠ¶ï¼Œä½†æ˜¯æ­¤å‡½æ•°å¹¶ä¸èƒ½ä¿è¯è¿”å›çš„æ˜¯å…¶æ‹·è´ï¼Œæ‰€ä»¥ä¸æ¨èä½¿ç”¨ã€‚æ¨èå…ˆç”¨`clone`åˆ›é€ ä¸€ä¸ªå‰¯æœ¬ç„¶åå†ä½¿ç”¨`view`ã€‚

#### è‡ªåŠ¨æ±‚æ¢¯åº¦

å°†`Tensor.requires_grad`è®¾ç½®ä¸º`True`ï¼Œå®ƒå°†å¼€å§‹è¿½è¸ª(track)åœ¨å…¶ä¸Šçš„æ‰€æœ‰æ“ä½œï¼ˆè¿™æ ·å°±å¯ä»¥åˆ©ç”¨é“¾å¼æ³•åˆ™è¿›è¡Œæ¢¯åº¦ä¼ æ’­äº†ï¼‰ã€‚å®Œæˆè®¡ç®—åï¼Œå¯ä»¥è°ƒç”¨`.backward()`æ¥å®Œæˆæ‰€æœ‰æ¢¯åº¦è®¡ç®—ã€‚æ­¤`Tensor`çš„æ¢¯åº¦å°†ç´¯ç§¯åˆ°`.grad`å±æ€§ä¸­ã€‚

`.backward()`å¦‚æœå‚æ•°æ˜¯æ ‡é‡ï¼Œå¯ä»¥ä¸ç”¨å†æ³¨æ˜æ±‚å¯¼å˜é‡ã€‚

å¦‚æœä¸æƒ³è¦è¢«ç»§ç»­è¿½è¸ªï¼Œå¯ä»¥è°ƒç”¨`.detach()`å°†å…¶ä»è¿½è¸ªè®°å½•ä¸­åˆ†ç¦»å‡ºæ¥ï¼Œè¿™æ ·å°±å¯ä»¥é˜²æ­¢å°†æ¥çš„è®¡ç®—è¢«è¿½è¸ªï¼Œè¿™æ ·æ¢¯åº¦å°±ä¼ ä¸è¿‡å»äº†ã€‚æ­¤å¤–ï¼Œè¿˜å¯ä»¥ç”¨`with torch.no_grad()`å°†ä¸æƒ³è¢«è¿½è¸ªçš„æ“ä½œä»£ç å—åŒ…è£¹èµ·æ¥ï¼Œè¿™ç§æ–¹æ³•åœ¨è¯„ä¼°æ¨¡å‹çš„æ—¶å€™å¾ˆå¸¸ç”¨ï¼Œå› ä¸ºåœ¨è¯„ä¼°æ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬å¹¶ä¸éœ€è¦è®¡ç®—å¯è®­ç»ƒå‚æ•°ï¼ˆ`requires_grad=True`ï¼‰çš„æ¢¯åº¦ã€‚

**åœ¨å¤šæ¬¡è¿­ä»£æ—¶æ³¨æ„æ¯æ¬¡è¿­ä»£æ¢¯åº¦æ¸…é›¶**ï¼š`.grad.data.zero_()`



### torch.nnå¸¸ç”¨å±‚

[torch.nnå®˜æ–¹æ–‡æ¡£](https://pytorch.org/docs/stable/nn.html#)

## é™„2-è¯¾ä½™è§é—»

### SOTA

state-of-the-artçš„è‹±æ–‡ç¼©å†™ï¼›

state-of-the-artå¸¸åœ¨å„ç§è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„ä¼šè®®æœŸåˆŠè®ºæ–‡ç¬¬ä¸€é¡µä¸Šè§åˆ°ï¼›

ç›´è¯‘æ„æ€æ˜¯ï¼š**æœ€å‰æ²¿çš„ï¼Œæœ€å…ˆè¿›çš„ï¼Œç›®å‰æœ€é«˜æ°´å¹³ï¼›**

å³æŒ‡åœ¨å…¬å¼€çš„æ•°æ®é›†ä¸Šï¼Œç›®å‰æ£€æµ‹åˆ°çš„æ•ˆæœæœ€å¥½ã€è¯†åˆ«ç‡æœ€é«˜ã€æ­£ç¡®ç‡æœ€é«˜ï¼Œç®—æ³•æ¨¡å‹è¶…è¿‡ä»¥å‰çš„æ‰€æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°æœ€ä¼˜ï¼›
